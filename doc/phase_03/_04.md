# Phase 03-4: Streaming ì‘ë‹µ

## ëª©í‘œ

í˜„ì¬ `st.spinner("Thinking...")` í›„ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— í‘œì‹œí•˜ëŠ” ë°©ì‹ì„ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ UXë¥¼ ê°œì„ í•œë‹¤.

---

## 0. ì„ í–‰ ë¦¬íŒ©í† ë§: invoke() ê³µí†µ ë¡œì§ ì¶”ì¶œ

> `stream()`ê³¼ `invoke()`ì˜ ì½”ë“œ ì¤‘ë³µì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ ì „ì— ê³µí†µ ë¡œì§ì„ ë¨¼ì € ì¶”ì¶œí•œë‹¤.

### 0.1 í˜„ì¬ ë¬¸ì œ

`invoke()`ì˜ ì´ˆê¸°í™” ë¡œì§(casual ë¶„ê¸°, ë©”ì‹œì§€ ë³€í™˜, initial_state êµ¬ì„±, normal_turn_ids ì²˜ë¦¬)ê³¼ ê²°ê³¼ íŒŒì‹± ë¡œì§(í˜„ì¬ í„´ ì¶”ì¶œ, tool_history, final_text)ì´ `stream()`ì—ì„œ ê·¸ëŒ€ë¡œ ë°˜ë³µëœë‹¤.

**ì˜í–¥**: Phase 03-3-2 casual mode ë„ì… ë•Œ ì´ë¯¸ ê²½í—˜í•œ ë™ê¸°í™” ë¬¸ì œì˜ ë°˜ë³µ.

### 0.2 _prepare_invocation() â€” ê³µí†µ ì „ì²˜ë¦¬

**íŒŒì¼**: `service/react_graph.py`

```python
def _prepare_invocation(
    self,
    user_input: str,
    session_id: str,
    messages: list = None,
    summary: str = "",
    pdf_description: str = "",
    turn_count: int = 0,
    summary_history: list = None,
    compression_rate: float = 0.3,
    normal_turn_ids: list = None,
) -> tuple[str, dict, dict, list] | None:
    """invoke()ì™€ stream() ê³µí†µ ì „ì²˜ë¦¬

    Returns:
        casualì¸ ê²½ìš°: None (í˜¸ì¶œì¸¡ì—ì„œ casual ì²˜ë¦¬)
        normalì¸ ê²½ìš°: ("normal", initial_state, config, updated_normal_turn_ids)
    """
    if normal_turn_ids is None:
        normal_turn_ids = []

    # casual ë¶„ê¸° íŒë‹¨
    from service.reasoning_detector import detect_reasoning_need
    mode = detect_reasoning_need(user_input)

    if mode == "casual":
        return None  # í˜¸ì¶œì¸¡ì—ì„œ casual ì²˜ë¦¬

    # normal ëª¨ë“œ
    updated_normal_turn_ids = normal_turn_ids + [turn_count]
    normal_turn_count = len(updated_normal_turn_ids)

    self._current_session_id = session_id
    if self._graph is None:
        self.build()

    # ë©”ì‹œì§€ ë³€í™˜ (domain.message.Message â†’ LangChain BaseMessage)
    converted_messages = []
    for msg in (messages or []):
        if hasattr(msg, "role") and hasattr(msg, "content"):
            if msg.role == "user":
                converted_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                converted_messages.append(AIMessage(content=msg.content))
        elif isinstance(msg, BaseMessage):
            converted_messages.append(msg)

    user_message = HumanMessage(
        content=user_input,
        additional_kwargs={"turn_id": turn_count, "mode": "normal"}
    )
    all_messages = converted_messages + [user_message]

    initial_state: ChatState = {
        "messages": all_messages,
        "session_id": session_id,
        "summary": summary,
        "summary_history": summary_history or [],
        "turn_count": turn_count,
        "compression_rate": compression_rate,
        "pdf_description": pdf_description,
        "input_tokens": 0,
        "output_tokens": 0,
        "model_used": self.model_name,
        "normal_turn_count": normal_turn_count,
        "normal_turn_ids": updated_normal_turn_ids,
    }

    config = {"configurable": {"thread_id": session_id}}

    return "normal", initial_state, config, updated_normal_turn_ids
```

### 0.3 _extract_current_turn_messages() â€” í˜„ì¬ í„´ ì¶”ì¶œ

> `invoke()` ë¼ì¸ 649-661ê³¼ `_extract_tool_results()`ì—ì„œ ì¤‘ë³µë˜ë˜ turn_id ê¸°ë°˜ ì¶”ì¶œ ë¡œì§ì„ í†µí•©.

```python
def _extract_current_turn_messages(
    self, result_messages: list, turn_count: int
) -> list:
    """í˜„ì¬ í„´ì˜ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ (turn_id ê¸°ë°˜)"""
    for i, msg in enumerate(result_messages):
        if (isinstance(msg, HumanMessage) and
            getattr(msg, "additional_kwargs", {}).get("turn_id") == turn_count):
            return result_messages[i:]

    # Fallback: ì „ì²´ ë°˜í™˜
    return result_messages
```

### 0.4 _parse_result() â€” ê³µí†µ ê²°ê³¼ íŒŒì‹±

```python
def _parse_result(
    self,
    result_messages: list,
    turn_count: int,
) -> dict:
    """invoke()ì™€ stream() done ì´ë²¤íŠ¸ì—ì„œ ê³µí†µ ì‚¬ìš©

    Returns:
        dict: text, tool_history, tool_results
    """
    # ìµœì¢… í…ìŠ¤íŠ¸
    final_message = result_messages[-1] if result_messages else None
    final_text = ""
    if final_message:
        raw_content = getattr(final_message, "content", "")
        final_text = extract_text_from_content(raw_content)

    # í˜„ì¬ í„´ ë„êµ¬ ì •ë³´
    current_turn_messages = self._extract_current_turn_messages(
        result_messages, turn_count
    )
    tool_history = []
    tool_results = {}
    for msg in current_turn_messages:
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            for tc in msg.tool_calls:
                tool_history.append(tc["name"])
        if hasattr(msg, "type") and msg.type == "tool":
            tool_results[msg.name] = msg.content

    return {
        "text": final_text,
        "tool_history": tool_history,
        "tool_results": tool_results,
    }
```

### 0.5 invoke() ë¦¬íŒ©í† ë§

> ê¸°ì¡´ `invoke()` (693ì¤„)ë¥¼ ê³µí†µ ë©”ì„œë“œ í™œìš©ìœ¼ë¡œ ì•½ 30ì¤„ë¡œ ì¶•ì†Œ.

```python
def invoke(self, user_input, session_id, messages=None, summary="",
           pdf_description="", turn_count=0, summary_history=None,
           compression_rate=0.3, normal_turn_ids=None) -> dict:
    if normal_turn_ids is None:
        normal_turn_ids = []

    # ê³µí†µ ì „ì²˜ë¦¬
    preparation = self._prepare_invocation(
        user_input, session_id, messages, summary, pdf_description,
        turn_count, summary_history, compression_rate, normal_turn_ids,
    )

    if preparation is None:
        return self._invoke_casual(
            user_input, summary, summary_history, normal_turn_ids
        )

    mode, initial_state, config, updated_normal_turn_ids = preparation

    result = self._graph.invoke(initial_state, config)

    # ê³µí†µ ê²°ê³¼ íŒŒì‹±
    result_messages = result.get("messages", [])
    parsed = self._parse_result(result_messages, turn_count)

    input_tokens = result.get("input_tokens", 0)
    output_tokens = result.get("output_tokens", 0)

    return {
        **parsed,
        "iteration": len(parsed["tool_history"]),
        "model_used": self.model_name,
        "summary": result.get("summary", ""),
        "summary_history": result.get("summary_history", []),
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
        "normal_turn_ids": updated_normal_turn_ids,
        "normal_turn_count": len(updated_normal_turn_ids),
        "error": None,
    }


def _invoke_casual(self, user_input, summary, summary_history, normal_turn_ids):
    """casual ëª¨ë“œ invoke (ê¸°ì¡´ ì½”ë“œ ë¶„ë¦¬)"""
    casual_prompt = f"""ì‚¬ìš©ìê°€ "{user_input}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì§§ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë¶„ì„ì´ë‚˜ ì„¤ëª… ì—†ì´ìš”."""

    content, in_tokens, out_tokens = self._invoke_llm_with_token_tracking(
        [HumanMessage(content=casual_prompt)]
    )

    return {
        "text": content,
        "tool_history": [],
        "tool_results": {},
        "iteration": 0,
        "model_used": self.model_name,
        "summary": summary,
        "summary_history": summary_history or [],
        "input_tokens": in_tokens,
        "output_tokens": out_tokens,
        "total_tokens": in_tokens + out_tokens,
        "is_casual": True,
        "normal_turn_ids": normal_turn_ids,
        "normal_turn_count": len(normal_turn_ids),
        "error": None,
    }
```

### 0.6 app.py: _create_graph_builder() íŒ©í† ë¦¬

> `handle_chat_message`ì™€ `handle_stream_message`ì—ì„œ ì„œë¹„ìŠ¤ êµ¬ì„± ë¡œì§ì´ í†µì§¸ë¡œ ì¤‘ë³µë˜ëŠ” ê²ƒì„ ë°©ì§€.

**íŒŒì¼**: `app.py`

```python
def _create_graph_builder(
    settings: dict,
    embed_repo: EmbeddingRepository,
) -> ReactGraphBuilder:
    """handle_chat_messageì™€ handle_stream_message ê³µí†µ íŒ©í† ë¦¬"""
    search_service = None
    embedding_service = None

    if settings.get("tavily_api_key"):
        from service.search_service import SearchService
        search_service = SearchService(api_key=settings["tavily_api_key"])

    if st.session_state.chunks:
        embedding_service = EmbeddingService(
            api_key=settings["gemini_api_key"],
            model=settings.get("embedding_model", "gemini-embedding-001"),
        )

    return ReactGraphBuilder(
        api_key=settings["gemini_api_key"],
        model=settings.get("model", "gemini-2.0-flash"),
        temperature=settings.get("temperature", 0.7),
        top_p=settings.get("top_p", 0.9),
        max_output_tokens=settings.get("max_output_tokens", 8192),
        seed=settings.get("seed"),
        max_iterations=settings.get("max_iterations", 5),
        search_service=search_service,
        embedding_service=embedding_service,
        embedding_repo=embed_repo if st.session_state.chunks else None,
        db_path=DB_PATH,
    )
```

`handle_chat_message` ìˆ˜ì •:

```python
def handle_chat_message(user_input, settings, embed_repo) -> dict:
    # ... í† í° ì œí•œ ì²´í¬, ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼) ...

    graph_builder = _create_graph_builder(settings, embed_repo)
    result = graph_builder.invoke(
        user_input=user_input,
        session_id=session_id,
        messages=st.session_state.messages[:-1],
        summary=st.session_state.summary,
        pdf_description=st.session_state.pdf_description,
        turn_count=turn_count,
        summary_history=st.session_state.summary_history,
        compression_rate=settings.get("compression_rate", 0.3),
        normal_turn_ids=st.session_state.normal_turn_ids,
    )

    # ... ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼) ...
    return result
```

---

## 1. í˜„ì¬ êµ¬ì¡° vs ëª©í‘œ êµ¬ì¡°

### 1.1 í˜„ì¬ êµ¬ì¡°

**íŒŒì¼**: `component/chat_tab.py` (line 110-112)

```python
with st.chat_message("assistant"):
    with st.spinner("Thinking..."):
        response = on_send(user_input)  # ì „ì²´ ì‘ë‹µ ëŒ€ê¸°

    if response:
        st.markdown(response.get("text", ""))  # í•œ ë²ˆì— í‘œì‹œ
```

**ë¬¸ì œì **:
- ì‚¬ìš©ìê°€ ë¹ˆ í™”ë©´ì—ì„œ ì˜¤ë˜ ê¸°ë‹¤ë¦¼
- ì‘ë‹µ ì‹œê°„ì´ ê¸¸ìˆ˜ë¡ ì²´ê° ì§€ì—° ì¦ê°€

### 1.2 ëª©í‘œ êµ¬ì¡°

```python
with st.chat_message("assistant"):
    response_placeholder = st.empty()
    full_response = ""

    for chunk in on_stream(user_input):
        if chunk["type"] == "token":
            full_response += chunk["content"]
            response_placeholder.markdown(full_response + "â–Œ")

    response_placeholder.markdown(full_response)
```

---

## 2. react_graph.py ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ

### 2.1 stream()

> `_prepare_invocation()` ì¬ì‚¬ìš©ìœ¼ë¡œ invoke()ì™€ ì½”ë“œ ì¤‘ë³µ ì œê±°.

```python
from typing import Generator

def stream(
    self,
    user_input: str,
    session_id: str,
    messages: list = None,
    summary: str = "",
    pdf_description: str = "",
    turn_count: int = 0,
    summary_history: list = None,
    compression_rate: float = 0.3,
    normal_turn_ids: list = None,
) -> Generator[dict, None, None]:
    """ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰

    Yields:
        dict: {"type": "token"|"tool_call"|"tool_result"|"thought"|"done", ...}
    """
    if normal_turn_ids is None:
        normal_turn_ids = []

    # ê³µí†µ ì „ì²˜ë¦¬ (invokeì™€ ë™ì¼)
    preparation = self._prepare_invocation(
        user_input, session_id, messages, summary, pdf_description,
        turn_count, summary_history, compression_rate, normal_turn_ids,
    )

    if preparation is None:
        yield from self._stream_casual(
            user_input, summary, summary_history, normal_turn_ids
        )
        return

    mode, initial_state, config, updated_normal_turn_ids = preparation

    # LangGraph stream_mode="messages" (ë™ê¸°, Streamlit í˜¸í™˜)
    tool_calls_buffer = []

    for chunk, metadata in self._graph.stream(
        initial_state, config, stream_mode="messages"
    ):
        for event in self._parse_message_chunk(chunk, metadata, tool_calls_buffer):
            yield event

    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ê³µí†µ ê²°ê³¼ íŒŒì‹±
    final_state = self._graph.get_state(config).values
    result_messages = final_state.get("messages", [])
    parsed = self._parse_result(result_messages, turn_count)

    yield {
        "type": "done",
        "metadata": {
            **parsed,
            "model_used": self.model_name,
            "summary": final_state.get("summary", ""),
            "summary_history": final_state.get("summary_history", []),
            "input_tokens": final_state.get("input_tokens", 0),
            "output_tokens": final_state.get("output_tokens", 0),
            "normal_turn_ids": updated_normal_turn_ids,
            "normal_turn_count": len(updated_normal_turn_ids),
        }
    }
```

### 2.2 _parse_message_chunk()

> ë©€í‹° ë„êµ¬ í˜¸ì¶œ ì‹œ ëª¨ë“  tool_callì„ ì²˜ë¦¬í•˜ë„ë¡ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.

```python
def _parse_message_chunk(
    self, chunk, metadata: dict, tool_calls_buffer: list
) -> list[dict]:
    """stream_mode="messages" ì´ë²¤íŠ¸ íŒŒì‹±

    Returns:
        list[dict]: íŒŒì‹±ëœ ì´ë²¤íŠ¸ ëª©ë¡ (0~Nê°œ)
    """
    from langchain_core.messages import AIMessageChunk, ToolMessage

    events = []

    if isinstance(chunk, AIMessageChunk):
        # í…ìŠ¤íŠ¸ í† í°
        content = extract_text_from_content(chunk.content)
        if content:
            events.append({"type": "token", "content": content})

        # ë„êµ¬ í˜¸ì¶œ ê°ì§€ (ëª¨ë“  tool_call ì²˜ë¦¬)
        if chunk.tool_call_chunks:
            for tc in chunk.tool_call_chunks:
                name = tc.get("name")
                if name:
                    tool_calls_buffer.append({"name": name})
                    events.append({"type": "tool_call", "name": name})

    elif isinstance(chunk, ToolMessage):
        events.append({
            "type": "tool_result",
            "name": getattr(chunk, "name", "unknown"),
            "content": str(chunk.content)[:500],
        })

    return events
```

### 2.3 _stream_casual()

> í† í° ì¶”ì : ë§ˆì§€ë§‰ ì²­í¬ì˜ `usage_metadata`ë§Œ ì‚¬ìš© (ì¤‘ë³µ ì¹´ìš´íŒ… ë°©ì§€).

```python
def _stream_casual(
    self,
    user_input: str,
    summary: str,
    summary_history: list,
    normal_turn_ids: list,
) -> Generator[dict, None, None]:
    """ì¼ìƒì  ëŒ€í™” ìŠ¤íŠ¸ë¦¬ë°"""
    casual_prompt = f"""ì‚¬ìš©ìê°€ "{user_input}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì§§ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë¶„ì„ì´ë‚˜ ì„¤ëª… ì—†ì´ìš”."""

    full_text = ""
    last_usage = None

    for chunk in self._llm.stream([HumanMessage(content=casual_prompt)]):
        content = extract_text_from_content(chunk.content) if chunk.content else ""
        if content:
            full_text += content
            yield {"type": "token", "content": content}

        # usage_metadataëŠ” ë§ˆì§€ë§‰ ì²­í¬ì—ë§Œ í¬í•¨ë¨
        if hasattr(chunk, "usage_metadata") and chunk.usage_metadata:
            last_usage = chunk.usage_metadata

    in_tokens = last_usage.get("input_tokens", 0) if last_usage else 0
    out_tokens = last_usage.get("output_tokens", 0) if last_usage else 0

    yield {
        "type": "done",
        "metadata": {
            "text": full_text,
            "tool_history": [],
            "tool_results": {},
            "model_used": self.model_name,
            "summary": summary,
            "summary_history": summary_history or [],
            "input_tokens": in_tokens,
            "output_tokens": out_tokens,
            "is_casual": True,
            "normal_turn_ids": normal_turn_ids,
            "normal_turn_count": len(normal_turn_ids),
        }
    }
```

---

## 3. chat_tab.py ìˆ˜ì •

### 3.1 í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½

**íŒŒì¼**: `component/chat_tab.py`

```python
def render_chat_tab(
    on_send: callable,
    on_stream: callable = None,        # ì¶”ê°€
    messages: list[Message],
    summary_history: list[dict] = None,
    turn_count: int = None,
    use_streaming: bool = True,        # ì¶”ê°€
) -> None:
```

### 3.2 ì…ë ¥ ì²˜ë¦¬ ë¶€ë¶„ ìˆ˜ì •

```python
if user_input:
    with chat_col:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            if use_streaming and on_stream:
                response = _handle_streaming_response(on_stream, user_input)
            else:
                with st.spinner("Thinking..."):
                    response = on_send(user_input)

            if response:
                # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì´ë¯¸ í‘œì‹œë¨, fallbackë§Œ í‘œì‹œ
                if not (use_streaming and on_stream):
                    st.markdown(response.get("text", ""))

                # ë„êµ¬ ì •ë³´
                if response.get("tool_calls"):
                    with st.expander("ğŸ”§ íˆ´ ì‚¬ìš© ì •ë³´", expanded=False):
                        for tool in response["tool_calls"]:
                            st.markdown(f"**{tool['name']}**")
                            if tool.get("result"):
                                st.code(tool["result"][:500], language=None)

                # ëª¨ë¸ ìƒì„¸
                if response.get("model_used"):
                    with st.expander("Details", expanded=False):
                        st.caption(f"Model: {response['model_used']}")
                        st.caption(f"Tokens: {response.get('input_tokens', 0)} in / {response.get('output_tokens', 0)} out")
```

### 3.3 _handle_streaming_response()

```python
def _handle_streaming_response(on_stream: callable, user_input: str) -> dict:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    response_placeholder = st.empty()
    status_placeholder = st.empty()

    full_response = ""
    tool_calls = []
    metadata = {}

    for chunk in on_stream(user_input):
        chunk_type = chunk.get("type")

        if chunk_type == "token":
            full_response += chunk.get("content", "")
            response_placeholder.markdown(full_response + "â–Œ")

        elif chunk_type == "thought":
            # Phase 03-5ì—ì„œ êµ¬í˜„
            pass

        elif chunk_type == "tool_call":
            tool_name = chunk.get("name", "unknown")
            status_placeholder.caption(f"ğŸ”§ {tool_name} í˜¸ì¶œ ì¤‘...")
            tool_calls.append({"name": tool_name, "result": None})

        elif chunk_type == "tool_result":
            tool_name = chunk.get("name")
            for tc in tool_calls:
                if tc["name"] == tool_name and tc["result"] is None:
                    tc["result"] = chunk.get("content", "")
                    break
            status_placeholder.empty()

        elif chunk_type == "done":
            metadata = chunk.get("metadata", {})
            status_placeholder.empty()

    response_placeholder.markdown(full_response)

    return {
        "text": full_response,
        "tool_calls": tool_calls,
        "tool_results": metadata.get("tool_results", {}),
        "model_used": metadata.get("model_used", ""),
        "summary": metadata.get("summary", ""),
        "summary_history": metadata.get("summary_history", []),
        "input_tokens": metadata.get("input_tokens", 0),
        "output_tokens": metadata.get("output_tokens", 0),
        "normal_turn_ids": metadata.get("normal_turn_ids", []),
        "normal_turn_count": metadata.get("normal_turn_count", 0),
    }
```

---

## 4. app.py ìˆ˜ì •

### 4.1 handle_stream_message

> `_create_graph_builder()` íŒ©í† ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ì„œë¹„ìŠ¤ êµ¬ì„± ì¤‘ë³µ ì œê±°.

```python
def handle_stream_message(
    user_input: str,
    settings: dict,
    embed_repo: EmbeddingRepository,
):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í•¸ë“¤ëŸ¬

    Yields:
        dict: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ (token, tool_call, tool_result, done)
    """
    if not settings.get("gemini_api_key"):
        yield {"type": "token", "content": "Gemini API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        yield {"type": "done", "metadata": {"error": True}}
        return

    session_id = st.session_state.current_session
    current_tokens = st.session_state.token_usage["total"]
    if current_tokens >= TOKEN_LIMIT:
        yield {"type": "token", "content": f"í† í° ì œí•œ({TOKEN_LIMIT_K}k)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."}
        yield {"type": "done", "metadata": {"error": True}}
        return

    turn_count = get_turn_count(st.session_state.messages) + 1

    user_msg = Message(turn_id=turn_count, role="user", content=user_input)
    st.session_state.messages.append(user_msg)

    # íŒ©í† ë¦¬ ì‚¬ìš© (handle_chat_messageì™€ ë™ì¼í•œ ì„œë¹„ìŠ¤ êµ¬ì„±)
    graph_builder = _create_graph_builder(settings, embed_repo)

    final_metadata = {}

    for chunk in graph_builder.stream(
        user_input=user_input,
        session_id=session_id,
        messages=st.session_state.messages[:-1],
        summary=st.session_state.summary,
        pdf_description=st.session_state.pdf_description,
        turn_count=turn_count,
        summary_history=st.session_state.summary_history,
        compression_rate=settings.get("compression_rate", 0.3),
        normal_turn_ids=st.session_state.normal_turn_ids,
    ):
        if chunk.get("type") == "done":
            final_metadata = chunk.get("metadata", {})
        yield chunk

    # done ì´ë²¤íŠ¸ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
    if final_metadata and not final_metadata.get("error"):
        if final_metadata.get("summary"):
            st.session_state.summary = final_metadata["summary"]
        if final_metadata.get("summary_history"):
            st.session_state.summary_history = final_metadata["summary_history"]
        if "normal_turn_ids" in final_metadata:
            st.session_state.normal_turn_ids = final_metadata["normal_turn_ids"]

        function_calls = [{"name": t} for t in final_metadata.get("tool_history", [])]
        assistant_msg = Message(
            turn_id=turn_count,
            role="assistant",
            content=final_metadata.get("text", ""),
            input_tokens=final_metadata.get("input_tokens", 0),
            output_tokens=final_metadata.get("output_tokens", 0),
            model_used=final_metadata.get("model_used", ""),
            function_calls=function_calls,
            tool_results=final_metadata.get("tool_results", {}),
        )
        st.session_state.messages.append(assistant_msg)

        st.session_state.token_usage["input"] += final_metadata.get("input_tokens", 0)
        st.session_state.token_usage["output"] += final_metadata.get("output_tokens", 0)
        total = final_metadata.get("input_tokens", 0) + final_metadata.get("output_tokens", 0)
        st.session_state.token_usage["total"] += total
```

### 4.2 render_chat_tab í˜¸ì¶œ ìˆ˜ì •

```python
render_chat_tab(
    on_send=lambda msg: handle_chat_message(msg, settings, embed_repo),
    on_stream=lambda msg: handle_stream_message(msg, settings, embed_repo),
    messages=st.session_state.messages,
    summary_history=st.session_state.summary_history,
    turn_count=get_turn_count(st.session_state.messages),
    use_streaming=True,
)
```

---

## 5. ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ìœ í˜• | ë‚´ìš© |
|------|----------|------|
| `service/react_graph.py` | ìˆ˜ì • | `_prepare_invocation()`, `_extract_current_turn_messages()`, `_parse_result()` ì¶”ì¶œ. `invoke()` ë¦¬íŒ©í† ë§. `stream()`, `_stream_casual()`, `_parse_message_chunk()` ì¶”ê°€ |
| `component/chat_tab.py` | ìˆ˜ì • | `on_stream`, `use_streaming` íŒŒë¼ë¯¸í„°. `_handle_streaming_response()` ì¶”ê°€ |
| `app.py` | ìˆ˜ì • | `_create_graph_builder()` íŒ©í† ë¦¬. `handle_stream_message()` ì¶”ê°€. `render_chat_tab` í˜¸ì¶œ ìˆ˜ì • |

---

## 6. ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ íƒ€ì…

| type | ì„¤ëª… | ë°ì´í„° |
|------|------|--------|
| `token` | LLM í† í° ì¶œë ¥ | `{"content": "..."}` |
| `tool_call` | ë„êµ¬ í˜¸ì¶œ ì‹œì‘ | `{"name": "..."}` |
| `tool_result` | ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ | `{"name": "...", "content": "..."}` |
| `thought` | ì‚¬ê³  ê³¼ì • (Phase 03-5) | `{"content": "..."}` |
| `done` | ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ | `{"metadata": {...}}` |

> `thought` íƒ€ì…ì€ Phase 03-5ì—ì„œ ì¶”ê°€. `_handle_streaming_response`ì— placeholderë§Œ ë°°ì¹˜.

---

## 7. ê¸°ìˆ  ì„ íƒ: stream_mode="messages"

### 7.1 ì™œ astream_eventsê°€ ì•„ë‹Œê°€

| ë°©ì‹ | ì¥ì  | ë‹¨ì  |
|------|------|------|
| `astream_events` | ì„¸ë°€í•œ ì´ë²¤íŠ¸ | async í•„ìš”, Streamlit ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ìœ„í—˜ |
| **`stream_mode="messages"`** | **ë™ê¸°, í† í° ë‹¨ìœ„, Streamlit í˜¸í™˜** | LangGraph 0.2.60+ í•„ìš” |

í˜„ì¬ LangGraph ë²„ì „(1.0.7)ì—ì„œ `stream_mode="messages"` ì§€ì› í™•ì¸ë¨.

### 7.2 ë™ì‘ ì›ë¦¬

```python
for chunk, metadata in graph.stream(input, config, stream_mode="messages"):
    # chunk: AIMessageChunk (í† í° ë‹¨ìœ„) ë˜ëŠ” ToolMessage (ì™„ë£Œ ì‹œ)
    # metadata: {"langgraph_node": "llm_node", "langgraph_step": 3, ...}
    pass
```

---

## 8. Fallback ì „ëµ

```python
if use_streaming and on_stream:
    response = _handle_streaming_response(on_stream, user_input)
else:
    with st.spinner("Thinking..."):
        response = on_send(user_input)
```

`use_streaming=False`ë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ invoke ë°©ì‹ìœ¼ë¡œ ë™ì‘.

---

## 9. í…ŒìŠ¤íŠ¸ ê³„íš

### 9.1 ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ (Section 0)

```python
def test_prepare_invocation_casual():
    """casual ì…ë ¥ ì‹œ None ë°˜í™˜"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    result = graph._prepare_invocation("ì•ˆë…•!", session_id="test")
    assert result is None


def test_prepare_invocation_normal():
    """normal ì…ë ¥ ì‹œ (mode, state, config, turn_ids) ë°˜í™˜"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    result = graph._prepare_invocation(
        "ì§€ê¸ˆ ëª‡ ì‹œì•¼?", session_id="test", turn_count=1
    )
    assert result is not None
    mode, state, config, turn_ids = result
    assert mode == "normal"
    assert turn_ids == [1]


def test_parse_result_with_tools():
    """_parse_resultê°€ tool_historyì™€ tool_resultsë¥¼ ì •í™•íˆ ì¶”ì¶œ"""
    # mock messagesë¡œ í…ŒìŠ¤íŠ¸
    pass


def test_invoke_refactored_matches_original():
    """ë¦¬íŒ©í† ë§ëœ invoke()ê°€ ê¸°ì¡´ê³¼ ë™ì¼í•œ ê²°ê³¼ ë°˜í™˜"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    result = graph.invoke("ì•ˆë…•!", session_id="test", turn_count=1)
    assert "text" in result
    assert "tool_history" in result
    assert "normal_turn_ids" in result
```

### 9.2 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸

```python
def test_stream_casual():
    """casual ìŠ¤íŠ¸ë¦¬ë°: token + done"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    chunks = list(graph.stream("ì•ˆë…•!", session_id="test"))

    token_chunks = [c for c in chunks if c["type"] == "token"]
    assert len(token_chunks) > 0
    assert chunks[-1]["type"] == "done"


def test_stream_with_tool():
    """ë„êµ¬ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë°"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    graph.build()
    chunks = list(graph.stream("ì§€ê¸ˆ ëª‡ ì‹œì•¼?", session_id="test", turn_count=1))

    tool_calls = [c for c in chunks if c["type"] == "tool_call"]
    assert len(tool_calls) > 0

    done = chunks[-1]
    assert "get_current_time" in done["metadata"]["tool_history"]


def test_stream_done_metadata_keys():
    """stream done metadataê°€ invoke ë°˜í™˜ê°’ê³¼ ë™ì¼í•œ í‚¤ í¬í•¨"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    chunks = list(graph.stream("ì•ˆë…•!", session_id="test"))
    metadata = chunks[-1]["metadata"]

    required_keys = [
        "text", "tool_history", "model_used", "summary",
        "summary_history", "input_tokens", "output_tokens",
        "normal_turn_ids", "normal_turn_count",
    ]
    for key in required_keys:
        assert key in metadata, f"Missing key: {key}"
```

### 9.3 UI í…ŒìŠ¤íŠ¸

1. ì•± ì‹¤í–‰ (`streamlit run app.py`)
2. ì±„íŒ… ì…ë ¥
3. í™•ì¸:
   - í† í°ì´ í•˜ë‚˜ì”© í‘œì‹œë˜ëŠ”ì§€
   - ì»¤ì„œ(â–Œ)ê°€ ë³´ì´ë‹¤ê°€ ì™„ë£Œ ì‹œ ì‚¬ë¼ì§€ëŠ”ì§€
   - ë„êµ¬ í˜¸ì¶œ ì‹œ "ğŸ”§ {tool_name} í˜¸ì¶œ ì¤‘..." í‘œì‹œ
   - ì™„ë£Œ í›„ Details expander ì •ìƒ í‘œì‹œ
   - `use_streaming=False`ë¡œ ë³€ê²½ ì‹œ ê¸°ì¡´ ë°©ì‹ ë™ì‘

---

## 10. Phase 03-5 ì—°ë™ ì°¸ê³ 

Phase 03-5ì—ì„œ `thinking_budget`ê³¼ `include_thoughts`ë¥¼ `ChatGoogleGenerativeAI`ì— ì„¤ì •í•˜ë©´, thinking í† í°ì´ ìŠ¤íŠ¸ë¦¬ë°ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ë©ë‹ˆë‹¤.

- `_parse_message_chunk()`ì—ì„œ thought íŒŒíŠ¸ ê°ì§€ â†’ `{"type": "thought"}` ì´ë²¤íŠ¸ ì¶”ê°€
- `_handle_streaming_response()`ì—ì„œ thought ìˆ˜ì§‘ â†’ `st.expander("ğŸ§  ì‚¬ê³  ê³¼ì •")`ì— í‘œì‹œ

ì´ ì—°ë™ì€ _04ì˜ ì´ë²¤íŠ¸ êµ¬ì¡°ê°€ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ _05ì—ì„œ ì¶”ê°€ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
