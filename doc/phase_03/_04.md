# Phase 03-4: Streaming ì‘ë‹µ

## ëª©í‘œ

í˜„ì¬ `st.spinner("Thinking...")` í›„ ì „ì²´ ì‘ë‹µì„ í•œ ë²ˆì— í‘œì‹œí•˜ëŠ” ë°©ì‹ì„ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ UXë¥¼ ê°œì„ í•œë‹¤.

---

## 1. í˜„ì¬ êµ¬ì¡° vs ëª©í‘œ êµ¬ì¡°

### 1.1 í˜„ì¬ êµ¬ì¡°

**íŒŒì¼**: `component/chat_tab.py` (line 76-81)

```python
with st.chat_message("assistant"):
    with st.spinner("Thinking..."):
        response = on_send(user_input)  # ì „ì²´ ì‘ë‹µ ëŒ€ê¸°

    if response:
        st.markdown(response.get("text", ""))  # í•œ ë²ˆì— í‘œì‹œ
```

**ë¬¸ì œì **:
- ì‚¬ìš©ìê°€ ë¹ˆ í™”ë©´ì—ì„œ ì˜¤ë˜ ê¸°ë‹¤ë¦¼
- ì‘ë‹µ ì‹œê°„ì´ ê¸¸ìˆ˜ë¡ ì²´ê° ì§€ì—° ì¦ê°€

### 1.2 ëª©í‘œ êµ¬ì¡°

```python
with st.chat_message("assistant"):
    response_placeholder = st.empty()
    full_response = ""

    # í† í° ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
    for chunk in on_stream(user_input):
        full_response += chunk
        response_placeholder.markdown(full_response + "â–Œ")

    response_placeholder.markdown(full_response)
```

---

## 2. react_graph.pyì— ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ ì¶”ê°€

### 2.1 stream ë©”ì„œë“œ ì¶”ê°€

**íŒŒì¼**: `service/react_graph.py`

```python
from typing import Generator

def stream(
    self,
    user_input: str,
    session_id: str,
    messages: list = None,
    summary: str = "",
    pdf_description: str = "",
    turn_count: int = 0,
    summary_history: list = None,
) -> Generator[dict, None, None]:
    """ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰

    Yields:
        dict: {"type": "token", "content": "..."} ë˜ëŠ”
              {"type": "tool_call", "name": "...", "args": {...}} ë˜ëŠ”
              {"type": "tool_result", "name": "...", "content": "..."} ë˜ëŠ”
              {"type": "done", "metadata": {...}}
    """
    # Fast-path: ì¼ìƒì  ëŒ€í™”
    from service.reasoning_detector import detect_reasoning_need
    mode = detect_reasoning_need(user_input)

    if mode == "casual":
        # ê°„ë‹¨í•œ LLM ìŠ¤íŠ¸ë¦¬ë°
        yield from self._stream_casual(user_input, summary, summary_history)
        return

    if self._graph is None:
        self.build()

    user_message = HumanMessage(content=user_input)
    all_messages = (messages or []) + [user_message]

    initial_state: ChatState = {
        "messages": all_messages,
        "session_id": session_id,
        "summary": summary,
        "summary_history": summary_history or [],
        "turn_count": turn_count,
        "pdf_description": pdf_description,
        "input_tokens": 0,
        "output_tokens": 0,
        "model_used": self.model_name,
    }

    config = {"configurable": {"thread_id": session_id}}

    # astream_events ì‚¬ìš© (ë™ê¸° wrapper)
    import asyncio

    async def _async_stream():
        async for event in self._graph.astream_events(initial_state, config, version="v2"):
            yield event

    # ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
    loop = asyncio.new_event_loop()
    try:
        async_gen = _async_stream()
        while True:
            try:
                event = loop.run_until_complete(async_gen.__anext__())
                parsed = self._parse_stream_event(event)
                if parsed:
                    yield parsed
            except StopAsyncIteration:
                break
    finally:
        loop.close()


def _stream_casual(
    self,
    user_input: str,
    summary: str,
    summary_history: list
) -> Generator[dict, None, None]:
    """ì¼ìƒì  ëŒ€í™” ìŠ¤íŠ¸ë¦¬ë°"""
    casual_prompt = f"""ì‚¬ìš©ìê°€ "{user_input}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì§§ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    total_tokens = {"input": 0, "output": 0}

    for chunk in self._llm.stream([HumanMessage(content=casual_prompt)]):
        if chunk.content:
            yield {"type": "token", "content": chunk.content}

        # í† í° ì¶”ì 
        if hasattr(chunk, "usage_metadata") and chunk.usage_metadata:
            total_tokens["input"] += chunk.usage_metadata.get("input_tokens", 0)
            total_tokens["output"] += chunk.usage_metadata.get("output_tokens", 0)

    yield {
        "type": "done",
        "metadata": {
            "summary": summary,
            "summary_history": summary_history or [],
            "input_tokens": total_tokens["input"],
            "output_tokens": total_tokens["output"],
            "is_casual": True,
        }
    }


def _parse_stream_event(self, event: dict) -> dict | None:
    """ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ íŒŒì‹±"""
    event_type = event.get("event")
    data = event.get("data", {})

    if event_type == "on_chat_model_stream":
        # í† í° ìŠ¤íŠ¸ë¦¬ë°
        chunk = data.get("chunk")
        if chunk and hasattr(chunk, "content") and chunk.content:
            return {"type": "token", "content": chunk.content}

    elif event_type == "on_tool_start":
        # ë„êµ¬ í˜¸ì¶œ ì‹œì‘
        return {
            "type": "tool_call",
            "name": data.get("name", "unknown"),
            "args": data.get("input", {}),
        }

    elif event_type == "on_tool_end":
        # ë„êµ¬ í˜¸ì¶œ ì™„ë£Œ
        return {
            "type": "tool_result",
            "name": data.get("name", "unknown"),
            "content": str(data.get("output", "")),
        }

    elif event_type == "on_chain_end":
        # ê·¸ë˜í”„ ì¢…ë£Œ
        if event.get("name") == "LangGraph":
            output = data.get("output", {})
            return {
                "type": "done",
                "metadata": {
                    "summary": output.get("summary", ""),
                    "summary_history": output.get("summary_history", []),
                    "input_tokens": output.get("input_tokens", 0),
                    "output_tokens": output.get("output_tokens", 0),
                }
            }

    return None
```

---

## 3. chat_tab.py ìˆ˜ì •

### 3.1 ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€

**íŒŒì¼**: `component/chat_tab.py`

**í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½**:

```python
def render_chat_tab(
    on_send: callable,
    on_stream: callable,  # ì¶”ê°€
    messages: list[Message],
    summary_history: list[dict] = None,
    use_streaming: bool = True,  # ì¶”ê°€
) -> None:
```

### 3.2 ì…ë ¥ ì²˜ë¦¬ ë¶€ë¶„ ìˆ˜ì •

**ê¸°ì¡´ ì½”ë“œ** (ì‚­ì œ):

```python
if user_input:
    with chat_col:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = on_send(user_input)

            if response:
                st.markdown(response.get("text", ""))
                # ...
```

**ìˆ˜ì • ì½”ë“œ**:

```python
if user_input:
    with chat_col:
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            if use_streaming and on_stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                response = _handle_streaming_response(on_stream, user_input)
            else:
                # ê¸°ì¡´ ë°©ì‹ (fallback)
                with st.spinner("Thinking..."):
                    response = on_send(user_input)

            if response:
                # ë„êµ¬ ì •ë³´ í‘œì‹œ
                if response.get("tool_calls"):
                    with st.expander("ğŸ”§ íˆ´ ì‚¬ìš© ì •ë³´", expanded=False):
                        for tool in response["tool_calls"]:
                            st.markdown(f"**{tool['name']}**")
                            if tool.get("result"):
                                st.code(tool["result"][:500], language=None)

                # ë©”íƒ€ ì •ë³´ í‘œì‹œ
                if response.get("model_used"):
                    with st.expander("Details", expanded=False):
                        st.caption(f"Model: {response['model_used']}")
                        st.caption(f"Tokens: {response.get('input_tokens', 0)} in / {response.get('output_tokens', 0)} out")
```

### 3.3 ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€

```python
def _handle_streaming_response(on_stream: callable, user_input: str) -> dict:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    response_placeholder = st.empty()
    status_placeholder = st.empty()

    full_response = ""
    tool_calls = []
    metadata = {}

    for chunk in on_stream(user_input):
        chunk_type = chunk.get("type")

        if chunk_type == "token":
            # í† í° ìŠ¤íŠ¸ë¦¬ë°
            full_response += chunk.get("content", "")
            response_placeholder.markdown(full_response + "â–Œ")

        elif chunk_type == "tool_call":
            # ë„êµ¬ í˜¸ì¶œ í‘œì‹œ
            tool_name = chunk.get("name", "unknown")
            status_placeholder.caption(f"ğŸ”§ {tool_name} í˜¸ì¶œ ì¤‘...")
            tool_calls.append({
                "name": tool_name,
                "args": chunk.get("args", {}),
                "result": None,
            })

        elif chunk_type == "tool_result":
            # ë„êµ¬ ê²°ê³¼ ì €ì¥
            tool_name = chunk.get("name")
            for tc in tool_calls:
                if tc["name"] == tool_name and tc["result"] is None:
                    tc["result"] = chunk.get("content", "")
                    break
            status_placeholder.empty()

        elif chunk_type == "done":
            # ì™„ë£Œ
            metadata = chunk.get("metadata", {})
            status_placeholder.empty()

    # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
    response_placeholder.markdown(full_response)

    return {
        "text": full_response,
        "tool_calls": tool_calls,
        "model_used": metadata.get("model_used", ""),
        "summary": metadata.get("summary", ""),
        "summary_history": metadata.get("summary_history", []),
        "input_tokens": metadata.get("input_tokens", 0),
        "output_tokens": metadata.get("output_tokens", 0),
    }
```

---

## 4. app.py ìˆ˜ì •

### 4.1 ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì „ë‹¬

**íŒŒì¼**: `app.py`

```python
# ê¸°ì¡´
render_chat_tab(
    on_send=handle_chat,
    messages=st.session_state.messages,
    summary_history=st.session_state.get("summary_history", []),
)

# ìˆ˜ì •
render_chat_tab(
    on_send=handle_chat,
    on_stream=handle_stream,  # ì¶”ê°€
    messages=st.session_state.messages,
    summary_history=st.session_state.get("summary_history", []),
    use_streaming=True,  # ì¶”ê°€
)
```

### 4.2 handle_stream í•¨ìˆ˜ ì¶”ê°€

```python
def handle_stream(user_input: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í•¸ë“¤ëŸ¬"""
    if not st.session_state.get("graph_builder"):
        yield {"type": "token", "content": "ê·¸ë˜í”„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        yield {"type": "done", "metadata": {}}
        return

    graph = st.session_state.graph_builder

    yield from graph.stream(
        user_input=user_input,
        session_id=st.session_state.get("current_session", "default"),
        messages=_get_langchain_messages(),
        summary=st.session_state.get("summary", ""),
        pdf_description=st.session_state.get("pdf_description", ""),
        turn_count=len(st.session_state.get("messages", [])) // 2,
        summary_history=st.session_state.get("summary_history", []),
    )
```

---

## 5. ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ìœ í˜• | ë‚´ìš© |
|------|----------|------|
| `service/react_graph.py` | ìˆ˜ì • | `stream()` ë©”ì„œë“œ ì¶”ê°€ |
| `component/chat_tab.py` | ìˆ˜ì • | ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ |
| `app.py` | ìˆ˜ì • | `handle_stream` í•¨ìˆ˜ ë° ì½œë°± ì¶”ê°€ |

---

## 6. ì‚­ì œë˜ëŠ” ì½”ë“œ

**`component/chat_tab.py`ì—ì„œ ì‚­ì œ**:

```python
# ê¸°ì¡´ spinner ë°©ì‹ (ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ëŒ€ì²´)
with st.spinner("Thinking..."):
    response = on_send(user_input)
```

---

## 7. ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ íƒ€ì…

| type | ì„¤ëª… | ë°ì´í„° |
|------|------|--------|
| `token` | LLM í† í° ì¶œë ¥ | `{"content": "..."}` |
| `tool_call` | ë„êµ¬ í˜¸ì¶œ ì‹œì‘ | `{"name": "...", "args": {...}}` |
| `tool_result` | ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ | `{"name": "...", "content": "..."}` |
| `done` | ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ | `{"metadata": {...}}` |

---

## 8. í…ŒìŠ¤íŠ¸ ê³„íš

### 8.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
def test_stream_casual():
    """ì¼ìƒì  ëŒ€í™” ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))

    chunks = list(graph.stream("ì•ˆë…•!", session_id="test"))

    # token íƒ€ì…ì´ ìˆì–´ì•¼ í•¨
    token_chunks = [c for c in chunks if c["type"] == "token"]
    assert len(token_chunks) > 0

    # doneìœ¼ë¡œ ëë‚˜ì•¼ í•¨
    assert chunks[-1]["type"] == "done"


def test_stream_with_tool():
    """ë„êµ¬ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(api_key=os.getenv("GEMINI_API_KEY"))
    graph.build()

    chunks = list(graph.stream("ì§€ê¸ˆ ëª‡ ì‹œì•¼?", session_id="test"))

    # tool_call ì´ë²¤íŠ¸ í™•ì¸
    tool_calls = [c for c in chunks if c["type"] == "tool_call"]
    assert len(tool_calls) > 0
    assert tool_calls[0]["name"] == "get_current_time"
```

### 8.2 UI í…ŒìŠ¤íŠ¸

1. ì•± ì‹¤í–‰
2. ì±„íŒ… ì…ë ¥
3. í™•ì¸ ì‚¬í•­:
   - í† í°ì´ í•˜ë‚˜ì”© í‘œì‹œë˜ëŠ”ì§€
   - ì»¤ì„œ(â–Œ)ê°€ ë³´ì´ë‹¤ê°€ ì™„ë£Œ ì‹œ ì‚¬ë¼ì§€ëŠ”ì§€
   - ë„êµ¬ í˜¸ì¶œ ì‹œ ìƒíƒœ í‘œì‹œë˜ëŠ”ì§€

---

## 9. ì°¸ê³ : Streamlit ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´

```python
# ë°©ë²• 1: st.empty() + markdown
placeholder = st.empty()
for chunk in stream:
    text += chunk
    placeholder.markdown(text + "â–Œ")
placeholder.markdown(text)

# ë°©ë²• 2: st.write_stream (Streamlit 1.28+)
def generate():
    for chunk in stream:
        yield chunk["content"]

st.write_stream(generate())
```
