# Phase 03-6: 평가 시스템 (LangSmith Evaluation)

## 목표

LangSmith 기반 자동 평가 시스템을 구축하여 챗봇 응답 품질을 체계적으로 측정하고 개선한다.

---

## 1. LangSmith Evaluation 개요

### 1.1 평가 구성 요소

```
Dataset (테스트 케이스)
    ↓
Target Function (챗봇)
    ↓
Evaluators (평가자)
    ↓
Results (점수 + 피드백)
```

### 1.2 평가 흐름

1. **Dataset 생성**: 질문 + 예상 답변 쌍
2. **Target 정의**: 챗봇 호출 함수
3. **Evaluator 정의**: 점수 산출 로직
4. **evaluate() 실행**: 자동 평가
5. **결과 분석**: LangSmith 대시보드 또는 로컬

---

## 2. 프로젝트 구조

### 2.1 새 폴더 생성

```
evaluation/
├── __init__.py
├── datasets.py      # 데이터셋 정의
├── evaluators.py    # 평가자 정의
├── runner.py        # 평가 실행기
└── config.py        # 설정
```

---

## 3. 데이터셋 정의

### 3.1 evaluation/datasets.py

```python
"""평가용 데이터셋 정의"""
from langsmith import Client


def create_general_qa_dataset(client: Client) -> str:
    """일반 QA 데이터셋 생성

    Returns:
        str: 데이터셋 이름
    """
    dataset_name = "chatbot-general-qa"

    # 기존 데이터셋 확인
    try:
        client.read_dataset(dataset_name=dataset_name)
        return dataset_name
    except Exception:
        pass

    # 새 데이터셋 생성
    dataset = client.create_dataset(dataset_name=dataset_name)

    examples = [
        # 시간 관련
        {
            "inputs": {"question": "지금 몇 시야?"},
            "outputs": {"expected_tool": "get_current_time", "answer_contains": ["시", "분"]},
        },
        {
            "inputs": {"question": "오늘 날짜 알려줘"},
            "outputs": {"expected_tool": "get_current_time", "answer_contains": ["년", "월", "일"]},
        },

        # 일상 대화
        {
            "inputs": {"question": "안녕하세요"},
            "outputs": {"expected_tool": None, "answer_contains": ["안녕", "반갑"]},
        },
        {
            "inputs": {"question": "고마워"},
            "outputs": {"expected_tool": None, "answer_contains": []},  # 자연스러운 응답이면 OK
        },

        # 검색 필요
        {
            "inputs": {"question": "최신 AI 뉴스 알려줘"},
            "outputs": {"expected_tool": "web_search", "answer_contains": ["AI", "인공지능"]},
        },

        # 분석/추론
        {
            "inputs": {"question": "2+2는 뭐야?"},
            "outputs": {"expected_tool": None, "answer_contains": ["4"]},
        },
        {
            "inputs": {"question": "피보나치 수열의 10번째 항은?"},
            "outputs": {"expected_tool": "reasoning", "answer_contains": ["55"]},
        },
    ]

    client.create_examples(dataset_id=dataset.id, examples=examples)
    return dataset_name


def create_rag_dataset(client: Client, pdf_context: str = "") -> str:
    """RAG 평가용 데이터셋 생성

    Args:
        pdf_context: PDF 내용 요약 (테스트용)

    Returns:
        str: 데이터셋 이름
    """
    dataset_name = "chatbot-rag-qa"

    try:
        client.read_dataset(dataset_name=dataset_name)
        return dataset_name
    except Exception:
        pass

    dataset = client.create_dataset(dataset_name=dataset_name)

    # RAG 테스트 케이스 (실제 PDF 내용에 맞게 수정 필요)
    examples = [
        {
            "inputs": {"question": "문서에서 주요 내용을 요약해줘"},
            "outputs": {"expected_tool": "search_pdf_knowledge", "answer_contains": []},
        },
        {
            "inputs": {"question": "문서에 언급된 기술은 뭐가 있어?"},
            "outputs": {"expected_tool": "search_pdf_knowledge", "answer_contains": []},
        },
    ]

    client.create_examples(dataset_id=dataset.id, examples=examples)
    return dataset_name
```

---

## 4. 평가자 정의

### 4.1 evaluation/evaluators.py

```python
"""평가자 함수 정의"""
from typing import Any


def tool_usage_correct(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """도구 사용 정확도 평가

    Returns:
        dict: {"score": 0 or 1, "comment": "..."}
    """
    expected_tool = reference_outputs.get("expected_tool")
    actual_tools = outputs.get("tool_history", [])

    if expected_tool is None:
        # 도구 사용 안 해야 하는 경우
        score = 1 if len(actual_tools) == 0 else 0
        comment = "도구 미사용 OK" if score else f"불필요한 도구 사용: {actual_tools}"
    else:
        # 특정 도구 사용해야 하는 경우
        score = 1 if expected_tool in actual_tools else 0
        comment = f"기대: {expected_tool}, 실제: {actual_tools}"

    return {"score": score, "comment": comment}


def answer_contains_keywords(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """응답에 필수 키워드 포함 여부 평가

    Returns:
        dict: {"score": float 0~1, "comment": "..."}
    """
    expected_keywords = reference_outputs.get("answer_contains", [])
    answer = outputs.get("text", "")

    if not expected_keywords:
        return {"score": 1.0, "comment": "키워드 검사 불필요"}

    found = [kw for kw in expected_keywords if kw in answer]
    score = len(found) / len(expected_keywords) if expected_keywords else 1.0

    return {
        "score": score,
        "comment": f"발견: {found}, 누락: {set(expected_keywords) - set(found)}"
    }


def response_not_empty(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """응답이 비어있지 않은지 확인

    Returns:
        dict: {"score": 0 or 1, "comment": "..."}
    """
    answer = outputs.get("text", "")
    score = 1 if answer.strip() else 0

    return {
        "score": score,
        "comment": "응답 있음" if score else "빈 응답"
    }


def no_error(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """에러 없이 완료되었는지 확인

    Returns:
        dict: {"score": 0 or 1, "comment": "..."}
    """
    error = outputs.get("error")
    score = 1 if error is None else 0

    return {
        "score": score,
        "comment": "정상 완료" if score else f"에러: {error}"
    }


def token_efficiency(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict,
    max_tokens: int = 5000
) -> dict:
    """토큰 효율성 평가 (총 토큰 < max_tokens)

    Returns:
        dict: {"score": float 0~1, "comment": "..."}
    """
    total_tokens = outputs.get("total_tokens", 0)

    if total_tokens <= max_tokens:
        score = 1.0
    else:
        # 초과량에 따라 점수 감소
        score = max(0, 1 - (total_tokens - max_tokens) / max_tokens)

    return {
        "score": score,
        "comment": f"사용 토큰: {total_tokens} / 기준: {max_tokens}"
    }
```

---

## 5. 평가 실행기

### 5.1 evaluation/runner.py

```python
"""평가 실행기"""
import os
from langsmith import Client
from langsmith.evaluation import evaluate

from evaluation.datasets import create_general_qa_dataset
from evaluation.evaluators import (
    tool_usage_correct,
    answer_contains_keywords,
    response_not_empty,
    no_error,
    token_efficiency,
)


class EvaluationRunner:
    """LangSmith 평가 실행기"""

    def __init__(self, chatbot_invoke_fn: callable):
        """
        Args:
            chatbot_invoke_fn: 챗봇 호출 함수
                - 입력: {"question": "..."} 형태의 dict
                - 출력: {"text": "...", "tool_history": [...], ...} 형태의 dict
        """
        self.client = Client()
        self.chatbot = chatbot_invoke_fn

    def _target_function(self, inputs: dict) -> dict:
        """평가 대상 함수 래퍼"""
        question = inputs.get("question", "")
        result = self.chatbot(question)
        return result

    def run_general_evaluation(
        self,
        experiment_prefix: str = "chatbot-eval",
        max_concurrency: int = 2,
    ) -> dict:
        """일반 QA 평가 실행

        Args:
            experiment_prefix: 실험 이름 접두사
            max_concurrency: 동시 실행 수

        Returns:
            dict: 평가 결과 요약
        """
        # 데이터셋 생성/로드
        dataset_name = create_general_qa_dataset(self.client)

        # 평가 실행
        results = self.client.evaluate(
            self._target_function,
            data=dataset_name,
            evaluators=[
                tool_usage_correct,
                answer_contains_keywords,
                response_not_empty,
                no_error,
            ],
            experiment_prefix=experiment_prefix,
            max_concurrency=max_concurrency,
        )

        # 결과 집계
        return self._summarize_results(results)

    def run_efficiency_evaluation(
        self,
        questions: list[str],
        experiment_prefix: str = "chatbot-efficiency",
    ) -> dict:
        """토큰 효율성 평가

        Args:
            questions: 테스트 질문 리스트
            experiment_prefix: 실험 이름 접두사

        Returns:
            dict: 평가 결과 요약
        """
        # 임시 데이터셋 생성
        dataset_name = f"{experiment_prefix}-dataset"

        try:
            self.client.delete_dataset(dataset_name=dataset_name)
        except Exception:
            pass

        dataset = self.client.create_dataset(dataset_name=dataset_name)

        examples = [
            {"inputs": {"question": q}, "outputs": {}}
            for q in questions
        ]
        self.client.create_examples(dataset_id=dataset.id, examples=examples)

        # 평가 실행
        results = self.client.evaluate(
            self._target_function,
            data=dataset_name,
            evaluators=[
                token_efficiency,
                no_error,
            ],
            experiment_prefix=experiment_prefix,
        )

        return self._summarize_results(results)

    def _summarize_results(self, results) -> dict:
        """평가 결과 요약"""
        results_list = list(results)

        summary = {
            "total_examples": len(results_list),
            "evaluator_scores": {},
        }

        # 평가자별 평균 점수 계산
        for r in results_list:
            eval_results = r.get("evaluation_results", {}).get("results", [])
            for er in eval_results:
                key = er.key
                score = er.score

                if key not in summary["evaluator_scores"]:
                    summary["evaluator_scores"][key] = []
                summary["evaluator_scores"][key].append(score)

        # 평균 계산
        for key, scores in summary["evaluator_scores"].items():
            summary["evaluator_scores"][key] = {
                "mean": sum(scores) / len(scores) if scores else 0,
                "count": len(scores),
            }

        return summary
```

---

## 6. 사용 예시

### 6.1 evaluation/run_evaluation.py (스크립트)

```python
"""평가 실행 스크립트"""
import os
from dotenv import load_dotenv

load_dotenv()

from service.react_graph import ReactGraphBuilder
from evaluation.runner import EvaluationRunner


def main():
    # 챗봇 초기화
    graph_builder = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        temperature=0.7,
    )
    graph_builder.build()

    # 챗봇 호출 함수
    def chatbot_fn(question: str) -> dict:
        return graph_builder.invoke(
            user_input=question,
            session_id="evaluation",
        )

    # 평가 실행기 생성
    runner = EvaluationRunner(chatbot_fn)

    # 일반 QA 평가
    print("=" * 50)
    print("일반 QA 평가 실행 중...")
    results = runner.run_general_evaluation(
        experiment_prefix="chatbot-v3-general"
    )

    print("\n평가 결과:")
    print(f"  총 테스트: {results['total_examples']}")
    for evaluator, scores in results["evaluator_scores"].items():
        print(f"  {evaluator}: {scores['mean']:.2%} ({scores['count']}개)")

    # 토큰 효율성 평가
    print("\n" + "=" * 50)
    print("토큰 효율성 평가 실행 중...")
    efficiency_results = runner.run_efficiency_evaluation(
        questions=[
            "안녕하세요",
            "오늘 날씨 어때?",
            "AI의 역사에 대해 자세히 설명해줘",
            "피보나치 수열을 파이썬으로 구현해줘",
        ],
        experiment_prefix="chatbot-v3-efficiency"
    )

    print("\n효율성 평가 결과:")
    for evaluator, scores in efficiency_results["evaluator_scores"].items():
        print(f"  {evaluator}: {scores['mean']:.2%}")


if __name__ == "__main__":
    main()
```

---

## 7. 변경 파일 요약

| 파일 | 변경 유형 | 내용 |
|------|----------|------|
| `evaluation/__init__.py` | **신규** | 패키지 초기화 |
| `evaluation/datasets.py` | **신규** | 테스트 데이터셋 정의 |
| `evaluation/evaluators.py` | **신규** | 평가자 함수 정의 |
| `evaluation/runner.py` | **신규** | 평가 실행기 클래스 |
| `evaluation/run_evaluation.py` | **신규** | 실행 스크립트 |

---

## 8. 테스트 계획

### 8.1 평가자 단위 테스트

```python
def test_tool_usage_correct():
    from evaluation.evaluators import tool_usage_correct

    # 도구 사용 정확
    result = tool_usage_correct(
        inputs={"question": "지금 몇 시야?"},
        outputs={"tool_history": ["get_current_time"]},
        reference_outputs={"expected_tool": "get_current_time"}
    )
    assert result["score"] == 1

    # 도구 사용 오류
    result = tool_usage_correct(
        inputs={"question": "안녕"},
        outputs={"tool_history": ["web_search"]},
        reference_outputs={"expected_tool": None}
    )
    assert result["score"] == 0


def test_answer_contains_keywords():
    from evaluation.evaluators import answer_contains_keywords

    result = answer_contains_keywords(
        inputs={},
        outputs={"text": "지금 시간은 14시 30분입니다."},
        reference_outputs={"answer_contains": ["시", "분"]}
    )
    assert result["score"] == 1.0
```

### 8.2 통합 테스트

```bash
# 평가 실행
python -m evaluation.run_evaluation

# LangSmith 대시보드에서 결과 확인
# https://smith.langchain.com/projects/<project-name>/experiments
```

---

## 9. LangSmith 대시보드 활용

### 9.1 확인 가능한 정보

- **실험 비교**: 버전별 점수 비교
- **실패 케이스 분석**: 낮은 점수의 원인 파악
- **트레이스 상세**: 각 테스트의 전체 실행 과정
- **평가자별 분포**: 점수 히스토그램

### 9.2 CI/CD 연동 (선택)

```yaml
# .github/workflows/evaluate.yml
name: Chatbot Evaluation

on:
  push:
    branches: [main]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -e .

      - name: Run evaluation
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: chatbot-ci
        run: python -m evaluation.run_evaluation
```

---

## 10. 참고: 평가 기준 확장

필요 시 추가할 수 있는 평가자:

| 평가자 | 설명 |
|--------|------|
| `response_relevance` | 응답이 질문과 관련 있는지 (LLM-as-Judge) |
| `response_fluency` | 응답의 자연스러움 (LLM-as-Judge) |
| `hallucination_check` | 환각 여부 (RAG 컨텍스트와 대조) |
| `latency_check` | 응답 시간 체크 |
