# Phase 03-6: 평가 시스템 (LangSmith Evaluation)

## 목표

LangSmith 기반 자동 평가 시스템을 구축하여 챗봇 응답 품질을 체계적으로 측정하고 개선한다.

---

## 1. LangSmith Evaluation 개요

### 1.1 평가 구성 요소

```
Dataset (테스트 케이스)
    ↓
Target Function (챗봇)
    ↓
Evaluators (평가자)
    ↓
Results (점수 + 피드백)
```

### 1.2 평가 흐름

1. **Dataset 생성**: 질문 + 예상 답변 쌍
2. **Target 정의**: 챗봇 호출 함수
3. **Evaluator 정의**: 점수 산출 로직
4. **`evaluate()` 실행**: 자동 평가
5. **결과 분석**: LangSmith 대시보드 또는 로컬

---

## 2. 프로젝트 구조

### 2.1 새 폴더 생성

```
evaluation/
├── __init__.py
├── config.py         # 설정 (모델, 데이터셋 이름 등)
├── datasets.py       # 데이터셋 정의
├── evaluators.py     # 평가자 정의
├── runner.py         # 평가 실행기
├── test_data/        # 외부 테스트 데이터
│   └── general_qa.json
└── run_evaluation.py # 실행 스크립트
```

---

## 3. 설정 파일

### 3.1 evaluation/config.py

```python
"""평가 시스템 설정"""
import os

# LangSmith 설정 (.env에서 로드)
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY", "")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT", "gemini-hybrid-chatbot")

# 평가 대상 모델 설정
DEFAULT_MODEL = "gemini-2.5-flash"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_OUTPUT_TOKENS = 8192

# Phase 03-5: thinking 기본 설정 (평가 시)
DEFAULT_THINKING_BUDGET = 0
DEFAULT_SHOW_THOUGHTS = False

# 데이터셋 이름
GENERAL_QA_DATASET = "chatbot-general-qa"
RAG_QA_DATASET = "chatbot-rag-qa"

# 평가 기준
MAX_TOKENS_PER_RESPONSE = 5000
MAX_CONCURRENCY = 2
```

---

## 4. 데이터셋 정의

### 4.1 외부 JSON 파일로 분리

> 테스트 데이터를 코드에서 분리하여, 프롬프트나 도구 변경 시 데이터만 수정 가능.

**파일**: `evaluation/test_data/general_qa.json`

```json
[
    {
        "question": "지금 몇 시야?",
        "expected_tool": "get_current_time",
        "answer_contains": ["시", "분"]
    },
    {
        "question": "오늘 날짜 알려줘",
        "expected_tool": "get_current_time",
        "answer_contains": ["년", "월", "일"]
    },
    {
        "question": "안녕하세요",
        "expected_tool": null,
        "unexpected_tools": [],
        "answer_contains": []
    },
    {
        "question": "고마워",
        "expected_tool": null,
        "unexpected_tools": [],
        "answer_contains": []
    },
    {
        "question": "최신 AI 뉴스 알려줘",
        "expected_tool": "web_search",
        "answer_contains": []
    },
    {
        "question": "2+2는 뭐야?",
        "expected_tool": null,
        "answer_contains": ["4"]
    },
    {
        "question": "피보나치 수열의 10번째 항은?",
        "expected_tool": "reasoning",
        "answer_contains": ["55"]
    }
]
```

### 4.2 evaluation/datasets.py

```python
"""평가용 데이터셋 정의"""
import json
from pathlib import Path
from langsmith import Client


def _load_test_data(filename: str) -> list:
    """JSON 파일에서 테스트 데이터 로드"""
    data_path = Path(__file__).parent / "test_data" / filename
    if data_path.exists():
        with open(data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def create_general_qa_dataset(client: Client) -> str:
    """일반 QA 데이터셋 생성"""
    dataset_name = "chatbot-general-qa"

    try:
        client.read_dataset(dataset_name=dataset_name)
        return dataset_name
    except Exception:
        pass

    dataset = client.create_dataset(dataset_name=dataset_name)

    test_data = _load_test_data("general_qa.json")
    examples = [
        {
            "inputs": {"question": item["question"]},
            "outputs": {
                "expected_tool": item.get("expected_tool"),
                "unexpected_tools": item.get("unexpected_tools", []),
                "answer_contains": item.get("answer_contains", []),
            },
        }
        for item in test_data
    ]

    client.create_examples(dataset_id=dataset.id, examples=examples)
    return dataset_name


def create_rag_dataset(client: Client) -> str:
    """RAG 평가용 데이터셋 생성

    Note: PDF 업로드 → 임베딩 생성 → 평가 실행의 전체 파이프라인이 필요.
    현재는 placeholder. 실제 PDF 내용에 맞게 test_data/rag_qa.json 작성 필요.
    """
    dataset_name = "chatbot-rag-qa"

    try:
        client.read_dataset(dataset_name=dataset_name)
        return dataset_name
    except Exception:
        pass

    dataset = client.create_dataset(dataset_name=dataset_name)

    test_data = _load_test_data("rag_qa.json")
    if not test_data:
        # 기본 placeholder
        test_data = [
            {
                "question": "문서에서 주요 내용을 요약해줘",
                "expected_tool": "search_pdf_knowledge",
                "answer_contains": [],
            },
        ]

    examples = [
        {
            "inputs": {"question": item["question"]},
            "outputs": {
                "expected_tool": item.get("expected_tool"),
                "answer_contains": item.get("answer_contains", []),
            },
        }
        for item in test_data
    ]

    client.create_examples(dataset_id=dataset.id, examples=examples)
    return dataset_name
```

---

## 5. 평가자 정의

### 5.1 evaluation/evaluators.py

> `invoke()` 반환 구조: `tool_history` (list[str]), `text` (str), `total_tokens` (int), `error` (None | str)

```python
"""평가자 함수 정의"""


def tool_usage_correct(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """도구 사용 정확도 평가

    - expected_tool이 실제 사용되었는지 확인
    - unexpected_tools가 사용되지 않았는지 확인
    - 불필요한 추가 도구 사용도 감점
    """
    expected_tool = reference_outputs.get("expected_tool")
    unexpected_tools = reference_outputs.get("unexpected_tools", [])
    actual_tools = outputs.get("tool_history", [])

    if expected_tool is None:
        # 도구 미사용 기대
        score = 1 if len(actual_tools) == 0 else 0
        comment = "도구 미사용 OK" if score else f"불필요한 도구 사용: {actual_tools}"
    else:
        if expected_tool not in actual_tools:
            score = 0
            comment = f"기대 도구 미사용. 기대: {expected_tool}, 실제: {actual_tools}"
        elif unexpected_tools and any(t in actual_tools for t in unexpected_tools):
            used_unexpected = [t for t in unexpected_tools if t in actual_tools]
            score = 0.5
            comment = f"기대 도구 사용했으나 불필요 도구도 사용: {used_unexpected}"
        else:
            score = 1
            comment = f"기대: {expected_tool}, 실제: {actual_tools}"

    return {"score": score, "comment": comment}


def answer_contains_keywords(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """응답에 필수 키워드 포함 여부 평가"""
    expected_keywords = reference_outputs.get("answer_contains", [])
    answer = outputs.get("text", "")

    if not expected_keywords:
        return {"score": 1.0, "comment": "키워드 검사 불필요"}

    found = [kw for kw in expected_keywords if kw in answer]
    score = len(found) / len(expected_keywords) if expected_keywords else 1.0

    return {
        "score": score,
        "comment": f"발견: {found}, 누락: {set(expected_keywords) - set(found)}"
    }


def response_not_empty(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """응답이 비어있지 않은지 확인"""
    answer = outputs.get("text", "")
    score = 1 if answer.strip() else 0
    return {"score": score, "comment": "응답 있음" if score else "빈 응답"}


def no_error(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict
) -> dict:
    """에러 없이 완료되었는지 확인"""
    error = outputs.get("error")
    score = 1 if error is None else 0
    return {"score": score, "comment": "정상 완료" if score else f"에러: {error}"}


def token_efficiency(
    inputs: dict,
    outputs: dict,
    reference_outputs: dict,
) -> dict:
    """토큰 효율성 평가"""
    from evaluation.config import MAX_TOKENS_PER_RESPONSE

    total_tokens = outputs.get("total_tokens", 0)

    if total_tokens <= MAX_TOKENS_PER_RESPONSE:
        score = 1.0
    else:
        score = max(0, 1 - (total_tokens - MAX_TOKENS_PER_RESPONSE) / MAX_TOKENS_PER_RESPONSE)

    return {
        "score": round(score, 3),
        "comment": f"사용 토큰: {total_tokens} / 기준: {MAX_TOKENS_PER_RESPONSE}"
    }
```

---

## 6. 평가 실행기

### 6.1 evaluation/runner.py

> **세션 격리**: 각 평가 케이스마다 고유 session_id 사용. 이전 평가의 대화 이력이 다음 평가에 영향을 주지 않도록 함.

```python
"""평가 실행기"""
import uuid
from langsmith import Client
from langsmith.evaluation import evaluate

from evaluation.config import MAX_CONCURRENCY
from evaluation.datasets import create_general_qa_dataset
from evaluation.evaluators import (
    tool_usage_correct,
    answer_contains_keywords,
    response_not_empty,
    no_error,
    token_efficiency,
)


class EvaluationRunner:
    """LangSmith 평가 실행기"""

    def __init__(self, graph_builder):
        """
        Args:
            graph_builder: ReactGraphBuilder 인스턴스 (build() 완료 상태)
        """
        self.client = Client()
        self.graph_builder = graph_builder

    def _target_function(self, inputs: dict) -> dict:
        """평가 대상 함수 — 각 호출마다 독립 세션 사용

        세션 격리: uuid 기반 고유 session_id로 평가 간 독립성 보장.
        turn_count=1: 항상 첫 턴으로 실행 (요약 트리거 방지).
        """
        question = inputs.get("question", "")
        unique_session = f"eval-{uuid.uuid4().hex[:8]}"

        result = self.graph_builder.invoke(
            user_input=question,
            session_id=unique_session,
            turn_count=1,
        )
        return result

    def run_general_evaluation(
        self,
        experiment_prefix: str = "chatbot-eval",
        max_concurrency: int = None,
    ) -> dict:
        """일반 QA 평가 실행"""
        if max_concurrency is None:
            max_concurrency = MAX_CONCURRENCY

        dataset_name = create_general_qa_dataset(self.client)

        results = evaluate(
            self._target_function,
            data=dataset_name,
            evaluators=[
                tool_usage_correct,
                answer_contains_keywords,
                response_not_empty,
                no_error,
            ],
            experiment_prefix=experiment_prefix,
            max_concurrency=max_concurrency,
        )

        return self._summarize_results(results)

    def run_efficiency_evaluation(
        self,
        questions: list[str],
        experiment_prefix: str = "chatbot-efficiency",
    ) -> dict:
        """토큰 효율성 평가"""
        dataset_name = f"{experiment_prefix}-dataset"

        try:
            self.client.delete_dataset(dataset_name=dataset_name)
        except Exception:
            pass

        dataset = self.client.create_dataset(dataset_name=dataset_name)

        examples = [
            {"inputs": {"question": q}, "outputs": {}}
            for q in questions
        ]
        self.client.create_examples(dataset_id=dataset.id, examples=examples)

        results = evaluate(
            self._target_function,
            data=dataset_name,
            evaluators=[
                token_efficiency,
                no_error,
            ],
            experiment_prefix=experiment_prefix,
        )

        return self._summarize_results(results)

    def _summarize_results(self, results) -> dict:
        """평가 결과 요약

        ExperimentResults를 이터레이션하여 결과 집계.
        ExperimentResultRow의 실제 구조에 맞게 파싱.
        """
        results_list = list(results)

        summary = {
            "total_examples": len(results_list),
            "evaluator_scores": {},
        }

        for r in results_list:
            # ExperimentResultRow 구조에서 평가 결과 추출
            eval_results = getattr(r, "evaluation_results", None)
            if eval_results is None:
                # dict 형태 fallback
                eval_results = r.get("evaluation_results", {}) if isinstance(r, dict) else {}

            # results 리스트 추출
            if hasattr(eval_results, "results"):
                result_items = eval_results.results
            elif isinstance(eval_results, dict):
                result_items = eval_results.get("results", [])
            else:
                result_items = []

            for er in result_items:
                key = getattr(er, "key", None) or (er.get("key") if isinstance(er, dict) else "unknown")
                score = getattr(er, "score", None)
                if score is None and isinstance(er, dict):
                    score = er.get("score", 0)

                if key not in summary["evaluator_scores"]:
                    summary["evaluator_scores"][key] = []
                if score is not None:
                    summary["evaluator_scores"][key].append(score)

        # 평균 계산
        for key, scores in summary["evaluator_scores"].items():
            summary["evaluator_scores"][key] = {
                "mean": round(sum(scores) / len(scores), 3) if scores else 0,
                "count": len(scores),
            }

        return summary
```

---

## 7. 실행 스크립트

### 7.1 evaluation/run_evaluation.py

> Phase 03-5 파라미터 반영. 세션 격리 적용.

```python
"""평가 실행 스크립트

사용법:
    python -m evaluation.run_evaluation
"""
import os
from dotenv import load_dotenv

load_dotenv()

from service.react_graph import ReactGraphBuilder
from evaluation.runner import EvaluationRunner
from evaluation.config import (
    DEFAULT_MODEL, DEFAULT_TEMPERATURE, DEFAULT_MAX_OUTPUT_TOKENS,
    DEFAULT_THINKING_BUDGET, DEFAULT_SHOW_THOUGHTS,
)


def main():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("GEMINI_API_KEY가 설정되지 않았습니다.")
        return

    # 챗봇 초기화 (Phase 03-5 파라미터 포함)
    graph_builder = ReactGraphBuilder(
        api_key=api_key,
        model=DEFAULT_MODEL,
        temperature=DEFAULT_TEMPERATURE,
        max_output_tokens=DEFAULT_MAX_OUTPUT_TOKENS,
        thinking_budget=DEFAULT_THINKING_BUDGET,
        show_thoughts=DEFAULT_SHOW_THOUGHTS,
    )
    graph_builder.build()

    # 평가 실행기 생성 (graph_builder 직접 전달)
    runner = EvaluationRunner(graph_builder)

    # === 일반 QA 평가 ===
    print("=" * 50)
    print("일반 QA 평가 실행 중...")
    results = runner.run_general_evaluation(
        experiment_prefix="chatbot-v3-general"
    )

    print(f"\n총 테스트: {results['total_examples']}")
    for evaluator, scores in results["evaluator_scores"].items():
        print(f"  {evaluator}: {scores['mean']:.1%} ({scores['count']}개)")

    # === 토큰 효율성 평가 ===
    print("\n" + "=" * 50)
    print("토큰 효율성 평가 실행 중...")

    efficiency_results = runner.run_efficiency_evaluation(
        questions=[
            "안녕하세요",
            "오늘 날씨 어때?",
            "AI의 역사에 대해 자세히 설명해줘",
            "피보나치 수열을 파이썬으로 구현해줘",
        ],
        experiment_prefix="chatbot-v3-efficiency"
    )

    print(f"\n총 테스트: {efficiency_results['total_examples']}")
    for evaluator, scores in efficiency_results["evaluator_scores"].items():
        print(f"  {evaluator}: {scores['mean']:.1%}")


if __name__ == "__main__":
    main()
```

---

## 8. 변경 파일 요약

| 파일 | 변경 유형 | 내용 |
|------|----------|------|
| `evaluation/__init__.py` | **신규** | 패키지 초기화 |
| `evaluation/config.py` | **신규** | 모델/데이터셋/기준 설정 (Phase 03-5 파라미터 포함) |
| `evaluation/datasets.py` | **신규** | 데이터셋 정의 (JSON 외부 파일 로드) |
| `evaluation/evaluators.py` | **신규** | 평가자 함수 5개 |
| `evaluation/runner.py` | **신규** | 평가 실행기 (세션 격리, ExperimentResults 파싱) |
| `evaluation/run_evaluation.py` | **신규** | 실행 스크립트 (Phase 03-5 파라미터) |
| `evaluation/test_data/general_qa.json` | **신규** | 테스트 데이터 (외부 파일) |

> 기존 코드 변경 없음. evaluation 패키지만 추가.

---

## 9. 도구 매핑 (현재 코드와의 일치 확인)

현재 `service/tools.py`에 정의된 도구 목록:

| 도구 이름 | 데이터셋에서 참조 | 비고 |
|-----------|-------------------|------|
| `get_current_time` | O | 시간/날짜 질문 |
| `web_search` | O | 최신 정보 검색 |
| `search_pdf_knowledge` | O (RAG 데이터셋) | PDF 문서 검색 |
| `reasoning` | O | 분석/추론 질문 |

---

## 10. 테스트 계획

### 10.1 평가자 단위 테스트

```python
def test_tool_usage_correct():
    from evaluation.evaluators import tool_usage_correct

    # 도구 사용 정확
    result = tool_usage_correct(
        inputs={"question": "지금 몇 시야?"},
        outputs={"tool_history": ["get_current_time"]},
        reference_outputs={"expected_tool": "get_current_time"}
    )
    assert result["score"] == 1

    # 불필요한 도구 사용
    result = tool_usage_correct(
        inputs={"question": "안녕"},
        outputs={"tool_history": ["web_search"]},
        reference_outputs={"expected_tool": None}
    )
    assert result["score"] == 0

    # 도구 미사용 (정상)
    result = tool_usage_correct(
        inputs={"question": "안녕"},
        outputs={"tool_history": []},
        reference_outputs={"expected_tool": None}
    )
    assert result["score"] == 1

    # 기대 도구 사용 + 불필요 추가 도구
    result = tool_usage_correct(
        inputs={"question": "지금 몇 시야?"},
        outputs={"tool_history": ["get_current_time", "web_search"]},
        reference_outputs={
            "expected_tool": "get_current_time",
            "unexpected_tools": ["web_search"]
        }
    )
    assert result["score"] == 0.5


def test_answer_contains_keywords():
    from evaluation.evaluators import answer_contains_keywords

    result = answer_contains_keywords(
        inputs={},
        outputs={"text": "지금 시간은 14시 30분입니다."},
        reference_outputs={"answer_contains": ["시", "분"]}
    )
    assert result["score"] == 1.0

    # 부분 매칭
    result = answer_contains_keywords(
        inputs={},
        outputs={"text": "현재 14시입니다."},
        reference_outputs={"answer_contains": ["시", "분"]}
    )
    assert result["score"] == 0.5


def test_token_efficiency():
    from evaluation.evaluators import token_efficiency

    result = token_efficiency(
        inputs={},
        outputs={"total_tokens": 3000},
        reference_outputs={}
    )
    assert result["score"] == 1.0

    result = token_efficiency(
        inputs={},
        outputs={"total_tokens": 7500},
        reference_outputs={}
    )
    assert result["score"] < 1.0
    assert result["score"] > 0
```

### 10.2 세션 격리 테스트

```python
def test_session_isolation():
    """각 평가 호출이 독립 세션을 사용하는지 확인"""
    from evaluation.runner import EvaluationRunner

    graph_builder = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
    )
    graph_builder.build()

    runner = EvaluationRunner(graph_builder)

    # 두 번 호출해도 세션이 다름
    result1 = runner._target_function({"question": "안녕"})
    result2 = runner._target_function({"question": "안녕"})

    # 둘 다 정상 응답 (이전 세션 영향 없음)
    assert result1.get("text")
    assert result2.get("text")
    assert result1.get("error") is None
    assert result2.get("error") is None
```

### 10.3 통합 테스트

```bash
# 환경 변수 확인
echo $GEMINI_API_KEY
echo $LANGSMITH_API_KEY

# 평가 실행
python -m evaluation.run_evaluation

# LangSmith 대시보드에서 결과 확인
```

---

## 11. LangSmith 대시보드 활용

### 11.1 확인 가능한 정보

- **실험 비교**: 버전별 점수 비교 (v3-general vs v3.1-general)
- **실패 케이스 분석**: 낮은 점수의 원인 파악
- **트레이스 상세**: 각 테스트의 전체 실행 과정
- **평가자별 분포**: 점수 히스토그램

### 11.2 활용 시나리오

1. **프롬프트 변경 후**: 이전 실험과 점수 비교
2. **모델 변경 후**: gemini-2.5-flash vs gemini-2.5-pro 비교
3. **새 도구 추가 후**: 도구 사용 정확도 확인
4. **thinking_budget 조절 후**: 효율성 + 품질 trade-off 확인

---

## 12. RAG 평가 한계 (현재)

현재 `run_evaluation.py`에서 `EmbeddingService`, `EmbeddingRepository`를 주입하지 않으므로, `search_pdf_knowledge` 도구가 항상 "설정되지 않았습니다"를 반환한다.

**RAG 평가를 위한 전제 조건**:
1. 평가용 PDF 파일 준비
2. 평가 스크립트에서 PDF 업로드 → 임베딩 생성 파이프라인 구현
3. `evaluation/test_data/rag_qa.json`에 PDF 내용 기반 질문/답변 작성
4. `EmbeddingService`와 `EmbeddingRepository`를 `ReactGraphBuilder`에 주입

> RAG 평가는 별도 이슈로 관리. 일반 QA 평가와 효율성 평가를 먼저 안정화한 후 추가.

---

## 13. 참고: 평가 기준 확장

필요 시 추가할 수 있는 평가자:

| 평가자 | 설명 | 구현 방식 |
|--------|------|----------|
| `response_relevance` | 응답이 질문과 관련 있는지 | LLM-as-Judge |
| `response_fluency` | 응답의 자연스러움 | LLM-as-Judge |
| `hallucination_check` | 환각 여부 (RAG용) | LLM-as-Judge |
| `latency_check` | 응답 시간 체크 | time.time() 측정 |

> LLM-as-Judge 패턴은 비용이 추가되므로 필요 시 도입.
