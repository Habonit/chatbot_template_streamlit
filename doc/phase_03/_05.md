# Phase 03-5: ì¶”ë¡  ëª¨ë¸ (Thinking)

## ëª©í‘œ

Gemini 2.5ì˜ `thinking_budget` / `include_thoughts` ê¸°ëŠ¥ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ ë ˆë²¨ì—ì„œ ì¶”ë¡  ëª¨ë“œë¥¼ ì œì–´í•œë‹¤. `langchain-google-genai` 4.2.0ì˜ ë„¤ì´í‹°ë¸Œ Thinking ì§€ì›ì„ í™œìš©í•˜ì—¬, ê¸°ì¡´ ê·¸ë˜í”„ êµ¬ì¡° ë³€ê²½ ì—†ì´ LLM ì„¤ì •ë§Œìœ¼ë¡œ êµ¬í˜„í•œë‹¤.

---

## 1. í˜„ì¬ êµ¬ì¡°

### 1.1 í˜„ì¬ ì¶”ë¡  ëª¨ë“œ ì²˜ë¦¬

**íŒŒì¼**: `component/sidebar.py` (line 114-127)

```python
reasoning_mode = st.toggle("ì¶”ë¡  ëª¨ë“œ (Reasoning Mode)", value=False)
auto_reasoning = st.toggle("ìë™ ì¶”ë¡  ëª¨ë“œ ê°ì§€", value=True)
```

**ë¬¸ì œì **:
- ëª¨ë¸ ë³€ê²½ë§Œ í•  ë¿, ì‹¤ì œ thinking ê¸°ëŠ¥ ë¯¸ì‚¬ìš©
- `thinking_budget`, `include_thoughts` ë¯¸ì ìš©
- ì‚¬ê³  ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ì—†ìŒ

---

## 2. langchain-google-genai ë„¤ì´í‹°ë¸Œ Thinking ì§€ì›

> `langchain-google-genai` 4.2.0ì—ì„œ `ChatGoogleGenerativeAI`ê°€ thinkingì„ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›. ë³„ë„ SDKë‚˜ ë…¸ë“œ ì¶”ê°€ ë¶ˆí•„ìš”.

### 2.1 ChatGoogleGenerativeAI thinking íŒŒë¼ë¯¸í„°

```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Gemini 2.5 ëª¨ë¸ìš©
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    thinking_budget=1024,        # ì¶”ë¡  í† í° ì˜ˆì‚° (0=ë¹„í™œì„±í™”, 128+)
    include_thoughts=True,       # ì‚¬ê³  ê³¼ì • ì‘ë‹µì— í¬í•¨
    temperature=0.7,
    max_output_tokens=8192,
)

response = llm.invoke("3x + 7 = 22 ì¼ ë•Œ xëŠ”?")
print(response.content)
print(f"ì¶”ë¡  í† í°: {response.usage_metadata['output_token_details']['reasoning']}")
```

**í•µì‹¬**: `bind_tools()`ì™€ ì™„ì „ í˜¸í™˜. ë„êµ¬ í˜¸ì¶œ + thinkingì´ ë™ì‹œì— ë™ì‘.

### 2.2 ëª¨ë¸ë³„ ì œì•½ì‚¬í•­

| ëª¨ë¸ | thinking ì§€ì› | ìµœì†Œ budget | ë¹„ê³  |
|------|--------------|-------------|------|
| gemini-2.5-pro | O | 128 | budget=0 ë¶ˆê°€ |
| gemini-2.5-flash | O | 1 | budget=0ìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥ |
| gemini-2.0-flash | X | - | thinking ë¯¸ì§€ì› |

### 2.3 ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ thinking í† í°

```python
for chunk in llm.stream("ë³µì¡í•œ ì§ˆë¬¸..."):
    # thinking íŒŒíŠ¸ê°€ contentì— í¬í•¨ë¨
    # chunk.contentê°€ listì¸ ê²½ìš° thought í•„ë“œë¡œ êµ¬ë¶„
    pass
```

thinkingì´ í™œì„±í™”ëœ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ:
- ì‚¬ê³  ê³¼ì • í† í°ì´ ë¨¼ì € ìŠ¤íŠ¸ë¦¬ë°ë¨ (content listì— `thought: True` íŒŒíŠ¸)
- ì´í›„ ì‘ë‹µ í† í°ì´ ìŠ¤íŠ¸ë¦¬ë°ë¨ (content listì— `thought: False` íŒŒíŠ¸)
- `usage_metadata`ì˜ `output_token_details.reasoning`ìœ¼ë¡œ thinking í† í° ìˆ˜ í™•ì¸

---

## 3. ì„¤ê³„: ê¸°ì¡´ ì•„í‚¤í…ì²˜ì™€ì˜ í†µí•©

### 3.1 ê¸°ì¡´ ì„¤ê³„ì˜ ë¬¸ì œ (í”¼ë“œë°± ë°˜ì˜)

ì´ì „ ì„¤ê³„ì—ì„œëŠ” `thinking_node`ë¥¼ ë³„ë„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , `google-genai` SDKë¥¼ ì§ì ‘ ì‚¬ìš©í–ˆë‹¤:

| ë¬¸ì œ | ì˜í–¥ |
|------|------|
| LLM ì´ì¤‘ í˜¸ì¶œ (llm_node + thinking_node) | ë¹„ìš©/ì‹œê°„ 2ë°° |
| SDK ì´ì›í™” (langchain-google-genai + google-genai) | ë©”ì‹œì§€ ë³€í™˜ í•„ìš”, ê¸°ìˆ  ë¶€ì±„ |
| ToolMessage â†’ user role ë³€í™˜ ìœ„í—˜ | ëª¨ë¸ ì˜¤í•´ ê°€ëŠ¥ |
| thought_processë¥¼ ChatStateì— ì €ì¥ | ë¶ˆí•„ìš”í•œ DB ëˆ„ì  |

### 3.2 ìƒˆ ì„¤ê³„: LLM ì„¤ì •ë§Œìœ¼ë¡œ í•´ê²°

`langchain-google-genai` 4.2.0ì˜ ë„¤ì´í‹°ë¸Œ ì§€ì›ìœ¼ë¡œ ìœ„ ë¬¸ì œê°€ ì „ë¶€ í•´ì†Œëœë‹¤:

```
ê¸°ì¡´ ì„¤ê³„ (ì‚­ì œ):
llm_node (LangChain) â†’ thinking_node (google-genai SDK) â†’ END
                         â†‘ ë³„ë„ SDK, ë©”ì‹œì§€ ë³€í™˜, ì´ì¤‘ í˜¸ì¶œ

ìƒˆ ì„¤ê³„:
llm_node (LangChain + thinking_budget) â†’ END
  â†‘ ë‹¨ì¼ SDK, ë³€í™˜ ì—†ìŒ, 1íšŒ í˜¸ì¶œ
```

| í•­ëª© | ê¸°ì¡´ ì„¤ê³„ | ìƒˆ ì„¤ê³„ |
|------|----------|---------|
| LLM í˜¸ì¶œ íšŸìˆ˜ | 2íšŒ (llm_node + thinking_node) | 1íšŒ |
| SDK | langchain-google-genai + google-genai | langchain-google-genaië§Œ |
| ë©”ì‹œì§€ ë³€í™˜ | `_convert_messages_to_genai_contents()` í•„ìš” | ë¶ˆí•„ìš” |
| ê·¸ë˜í”„ êµ¬ì¡° ë³€ê²½ | thinking_node ì¶”ê°€, build() ë¶„ê¸° | **ë³€ê²½ ì—†ìŒ** |
| ChatState ë³€ê²½ | thought_process í•„ë“œ ì¶”ê°€ | **ë³€ê²½ ì—†ìŒ** |
| ì¶”ê°€ ì½”ë“œëŸ‰ | ~150ì¤„ | ~30ì¤„ |

### 3.3 ê·¸ë˜í”„ êµ¬ì¡°

**thinking ë¹„í™œì„±í™” / í™œì„±í™” ëª¨ë‘ ë™ì¼**:
```
START â†’ summary_node â†’ llm_node â‡„ tool_node â†’ END
```

thinkingì€ llm_node ë‚´ë¶€ì˜ LLM ì„¤ì •ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ ê·¸ë˜í”„ í† í´ë¡œì§€ ë³€ê²½ì´ ì—†ë‹¤.

---

## 4. sidebar.py ìˆ˜ì •

### 4.1 thinking_budget ìŠ¬ë¼ì´ë” ì¶”ê°€

**ìœ„ì¹˜**: ì¶”ë¡  ëª¨ë“œ í† ê¸€ ì•„ë˜

```python
# Phase 02-7: ì¶”ë¡  ëª¨ë“œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
reasoning_mode = st.toggle(
    "ì¶”ë¡  ëª¨ë“œ (Reasoning Mode)",
    value=False,
    help="ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ì— thinking í™œì„±í™”",
)

# Phase 03-5: thinking ì„¤ì •
if reasoning_mode:
    thinking_budget = st.slider(
        "Thinking Budget",
        min_value=0,
        max_value=8192,
        value=1024,
        step=128,
        help="ì¶”ë¡ ì— ì‚¬ìš©í•  í† í° ì˜ˆì‚° (0: ë¹„í™œì„±í™”, 128+: í™œì„±í™”)",
    )

    show_thoughts = st.toggle(
        "ì¶”ë¡  ê³¼ì • í‘œì‹œ",
        value=False,
        help="ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •ì„ UIì— í‘œì‹œ",
    )

    st.caption(f"ğŸ“Š Thinking budget: {thinking_budget} tokens")
else:
    thinking_budget = 0
    show_thoughts = False
```

### 4.2 ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ ìˆ˜ì •

```python
return {
    # ê¸°ì¡´ í•­ëª©ë“¤ ì „ë¶€ ìœ ì§€...
    "reasoning_mode": reasoning_mode,
    "compression_rate": compression_rate,
    # Phase 03-5: thinking ì„¤ì • ì¶”ê°€
    "thinking_budget": thinking_budget,
    "show_thoughts": show_thoughts,
}
```

> `auto_reasoning` í† ê¸€ì€ Phase 03-7 (ë ˆê±°ì‹œ ì •ë¦¬)ì—ì„œ ì œê±° ì˜ˆì •. í˜„ì¬ëŠ” ìœ ì§€.

---

## 5. react_graph.py ìˆ˜ì •

### 5.1 __init__ íŒŒë¼ë¯¸í„° ì¶”ê°€

```python
def __init__(
    self,
    api_key: str,
    model: str = "gemini-2.0-flash",
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_output_tokens: int = 8192,
    seed: int = None,
    max_iterations: int = 5,
    thinking_budget: int = 0,      # Phase 03-5
    show_thoughts: bool = False,   # Phase 03-5
    search_service: Any = None,
    embedding_service: Any = None,
    embedding_repo: Any = None,
    db_path: str = None,
):
    # ê¸°ì¡´ ì´ˆê¸°í™”...
    self.thinking_budget = thinking_budget
    self.show_thoughts = show_thoughts

    # LLM ì´ˆê¸°í™” (thinking íŒŒë¼ë¯¸í„° í¬í•¨)
    llm_kwargs = {
        "model": model,
        "google_api_key": api_key,
        "temperature": temperature,
        "top_p": top_p,
        "max_output_tokens": max_output_tokens,
    }
    if seed is not None:
        llm_kwargs["seed"] = seed

    # Phase 03-5: thinking ì„¤ì •
    if thinking_budget > 0:
        llm_kwargs["thinking_budget"] = thinking_budget
        if show_thoughts:
            llm_kwargs["include_thoughts"] = True

    self._llm = ChatGoogleGenerativeAI(**llm_kwargs)
```

### 5.2 ëª¨ë¸ ì²´í¬

```python
    # Phase 03-5: ëª¨ë¸ í˜¸í™˜ì„± ì²´í¬
    THINKING_SUPPORTED_MODELS = ["gemini-2.5-pro", "gemini-2.5-flash"]

    if thinking_budget > 0 and model not in THINKING_SUPPORTED_MODELS:
        import warnings
        warnings.warn(
            f"{model}ì€ thinkingì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. thinking_budget ë¬´ì‹œë¨."
        )
        self.thinking_budget = 0
```

### 5.3 build() â€” ë³€ê²½ ì—†ìŒ

```python
def build(self):
    """ê·¸ë˜í”„ ë¹Œë“œ â€” Phase 03-5ì—ì„œ ë³€ê²½ ì‚¬í•­ ì—†ìŒ

    thinkingì€ LLM ì„¤ì •ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ ê·¸ë˜í”„ êµ¬ì¡° ë™ì¼.
    """
    self._tools = create_tools_with_services(...)
    self._llm_with_tools = self._llm.bind_tools(self._tools)
    tool_node = ToolNode(self._tools)

    builder = StateGraph(ChatState)
    builder.add_node("summary_node", self._summary_node)
    builder.add_node("llm_node", self._llm_node)
    builder.add_node("tool_node", tool_node)

    builder.add_edge(START, "summary_node")
    builder.add_edge("summary_node", "llm_node")
    builder.add_conditional_edges(
        "llm_node", tools_condition,
        {"tools": "tool_node", END: END}
    )
    builder.add_edge("tool_node", "llm_node")

    self._graph = builder.compile(checkpointer=self._checkpointer)
    return self._graph
```

### 5.4 ì‚¬ê³  ê³¼ì • ì¶”ì¶œ í—¬í¼

> `include_thoughts=True`ì¼ ë•Œ, `AIMessage.content`ê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë˜ë©° ê° íŒŒíŠ¸ì— `thought` í”Œë˜ê·¸ê°€ í¬í•¨ëœë‹¤.

```python
def extract_thought_from_content(content) -> tuple[str, str]:
    """AIMessage.contentì—ì„œ ì‚¬ê³  ê³¼ì •ê³¼ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬

    include_thoughts=True ì‹œ contentê°€ list:
      [{"type": "text", "text": "...", "thought": True}, ...]

    Returns:
        (thought_text, response_text)
    """
    if not isinstance(content, list):
        return "", str(content) if content else ""

    thought_parts = []
    response_parts = []

    for item in content:
        if isinstance(item, dict):
            text = item.get("text", "")
            if item.get("thought"):
                thought_parts.append(text)
            elif item.get("type") == "text":
                response_parts.append(text)
        elif isinstance(item, str):
            response_parts.append(item)

    return "\n".join(thought_parts), "".join(response_parts)
```

### 5.5 _parse_result() í™•ì¥

> Phase 03-4ì˜ `_parse_result()`ì— thought_process ì¶”ì¶œì„ ì¶”ê°€.

```python
def _parse_result(self, result_messages, turn_count) -> dict:
    """Phase 03-4 ê¸°ì¡´ ì½”ë“œ + Phase 03-5 thought ì¶”ì¶œ"""
    final_message = result_messages[-1] if result_messages else None
    final_text = ""
    thought_process = ""

    if final_message:
        raw_content = getattr(final_message, "content", "")

        if self.show_thoughts:
            thought_process, final_text = extract_thought_from_content(raw_content)
        else:
            final_text = extract_text_from_content(raw_content)

    # í˜„ì¬ í„´ ë„êµ¬ ì •ë³´ (ê¸°ì¡´ê³¼ ë™ì¼)
    current_turn_messages = self._extract_current_turn_messages(
        result_messages, turn_count
    )
    tool_history = []
    tool_results = {}
    for msg in current_turn_messages:
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            for tc in msg.tool_calls:
                tool_history.append(tc["name"])
        if hasattr(msg, "type") and msg.type == "tool":
            tool_results[msg.name] = msg.content

    result = {
        "text": final_text,
        "tool_history": tool_history,
        "tool_results": tool_results,
    }

    if thought_process:
        result["thought_process"] = thought_process

    return result
```

### 5.6 invoke() ë°˜í™˜ê°’ì— thought_process í¬í•¨

```python
def invoke(self, ...) -> dict:
    # ... _prepare_invocation, _graph.invoke, _parse_result (Phase 03-4ì™€ ë™ì¼) ...

    return {
        **parsed,
        # ... ê¸°ì¡´ í•„ë“œ ...
        "thought_process": parsed.get("thought_process", ""),  # Phase 03-5
        "error": None,
    }
```

---

## 6. Streaming + Thinking í†µí•©

### 6.1 _parse_message_chunk() í™•ì¥

Phase 03-4ì˜ `_parse_message_chunk()`ì— thought íŒŒíŠ¸ ê°ì§€ë¥¼ ì¶”ê°€.

```python
def _parse_message_chunk(
    self, chunk, metadata: dict, tool_calls_buffer: list
) -> list[dict]:
    """Phase 03-4 ê¸°ì¡´ ì½”ë“œ + Phase 03-5 thought ì²˜ë¦¬"""
    from langchain_core.messages import AIMessageChunk, ToolMessage

    events = []

    if isinstance(chunk, AIMessageChunk):
        # Phase 03-5: thought íŒŒíŠ¸ ê°ì§€
        if self.show_thoughts and isinstance(chunk.content, list):
            for item in chunk.content:
                if isinstance(item, dict):
                    text = item.get("text", "")
                    if not text:
                        continue
                    if item.get("thought"):
                        events.append({"type": "thought", "content": text})
                    elif item.get("type") == "text":
                        events.append({"type": "token", "content": text})
            if events:
                # thought/token ì´ë²¤íŠ¸ê°€ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©´ ë„êµ¬ ì²´í¬ ê±´ë„ˆëœ€
                return events

        # í…ìŠ¤íŠ¸ í† í° (ê¸°ì¡´)
        content = extract_text_from_content(chunk.content)
        if content:
            events.append({"type": "token", "content": content})

        # ë„êµ¬ í˜¸ì¶œ ê°ì§€ (ê¸°ì¡´)
        if chunk.tool_call_chunks:
            for tc in chunk.tool_call_chunks:
                name = tc.get("name")
                if name:
                    tool_calls_buffer.append({"name": name})
                    events.append({"type": "tool_call", "name": name})

    elif isinstance(chunk, ToolMessage):
        events.append({
            "type": "tool_result",
            "name": getattr(chunk, "name", "unknown"),
            "content": str(chunk.content)[:500],
        })

    return events
```

### 6.2 stream() done ì´ë²¤íŠ¸ì— thought_process í¬í•¨

```python
# stream() ë©”ì„œë“œì˜ done ì´ë²¤íŠ¸
yield {
    "type": "done",
    "metadata": {
        **parsed,  # _parse_result()ì—ì„œ thought_process í¬í•¨
        "model_used": self.model_name,
        # ... ê¸°ì¡´ í•„ë“œ ...
    }
}
```

### 6.3 _stream_casualì— thinking ë¯¸ì ìš©

casual ëŒ€í™”ì—ëŠ” thinkingì´ í•„ìš” ì—†ìŒ. `_stream_casual()`ì€ `self._llm`ì„ ì§ì ‘ ì‚¬ìš©í•˜ë¯€ë¡œ, thinkingì´ ì„¤ì •ëœ LLMì´ casualì—ì„œë„ ì‚¬ìš©ëœë‹¤. ê·¸ëŸ¬ë‚˜ casual í”„ë¡¬í”„íŠ¸ëŠ” ë‹¨ìˆœí•œ ì‘ë‹µë§Œ ìš”êµ¬í•˜ë¯€ë¡œ, ëª¨ë¸ì´ thinkingì„ ìµœì†Œí™”í•œë‹¤.

> thinking_budgetì´ ë†’ì•„ë„ casual ì‘ë‹µì—ì„œëŠ” ì‹¤ì œ thinking í† í°ì´ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŒ.

---

## 7. chat_tab.py â€” ì‚¬ê³  ê³¼ì • í‘œì‹œ

### 7.1 _handle_streaming_response ìˆ˜ì •

Phase 03-4ì˜ ì½”ë“œì— thought ìˆ˜ì§‘ ë° í‘œì‹œë¥¼ ì¶”ê°€.

```python
def _handle_streaming_response(on_stream: callable, user_input: str) -> dict:
    """Phase 03-4 ê¸°ì¡´ ì½”ë“œ + Phase 03-5 thought í‘œì‹œ"""
    response_placeholder = st.empty()
    status_placeholder = st.empty()
    thought_placeholder = st.empty()  # Phase 03-5

    full_response = ""
    full_thought = ""  # Phase 03-5
    tool_calls = []
    metadata = {}

    for chunk in on_stream(user_input):
        chunk_type = chunk.get("type")

        if chunk_type == "token":
            full_response += chunk.get("content", "")
            response_placeholder.markdown(full_response + "â–Œ")

        elif chunk_type == "thought":
            # Phase 03-5: ì‚¬ê³  ê³¼ì • ì‹¤ì‹œê°„ í‘œì‹œ
            full_thought += chunk.get("content", "")
            thought_placeholder.caption("ğŸ§  ì‚¬ê³  ì¤‘...")

        elif chunk_type == "tool_call":
            tool_name = chunk.get("name", "unknown")
            status_placeholder.caption(f"ğŸ”§ {tool_name} í˜¸ì¶œ ì¤‘...")
            tool_calls.append({"name": tool_name, "result": None})

        elif chunk_type == "tool_result":
            tool_name = chunk.get("name")
            for tc in tool_calls:
                if tc["name"] == tool_name and tc["result"] is None:
                    tc["result"] = chunk.get("content", "")
                    break
            status_placeholder.empty()

        elif chunk_type == "done":
            metadata = chunk.get("metadata", {})
            status_placeholder.empty()
            thought_placeholder.empty()

    response_placeholder.markdown(full_response)

    # Phase 03-5: ì‚¬ê³  ê³¼ì • expander (done í›„)
    thought_process = full_thought or metadata.get("thought_process", "")
    if thought_process:
        with st.expander("ğŸ§  ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •", expanded=False):
            st.markdown(thought_process)

    return {
        "text": full_response,
        "tool_calls": tool_calls,
        "tool_results": metadata.get("tool_results", {}),
        "model_used": metadata.get("model_used", ""),
        "summary": metadata.get("summary", ""),
        "summary_history": metadata.get("summary_history", []),
        "input_tokens": metadata.get("input_tokens", 0),
        "output_tokens": metadata.get("output_tokens", 0),
        "normal_turn_ids": metadata.get("normal_turn_ids", []),
        "normal_turn_count": metadata.get("normal_turn_count", 0),
        "thought_process": thought_process,  # Phase 03-5
    }
```

---

## 8. app.py ìˆ˜ì •

### 8.1 _create_graph_builder() íŒ©í† ë¦¬ì— thinking íŒŒë¼ë¯¸í„° ì¶”ê°€

> Phase 03-4ì—ì„œ ì¶”ì¶œí•œ `_create_graph_builder()`ì— thinking_budget, show_thoughts ì¶”ê°€.

```python
def _create_graph_builder(settings, embed_repo) -> ReactGraphBuilder:
    """Phase 03-4 íŒ©í† ë¦¬ + Phase 03-5 thinking íŒŒë¼ë¯¸í„°"""
    # ... ì„œë¹„ìŠ¤ êµ¬ì„± (Phase 03-4ì™€ ë™ì¼) ...

    return ReactGraphBuilder(
        api_key=settings["gemini_api_key"],
        model=settings.get("model", "gemini-2.0-flash"),
        temperature=settings.get("temperature", 0.7),
        top_p=settings.get("top_p", 0.9),
        max_output_tokens=settings.get("max_output_tokens", 8192),
        seed=settings.get("seed"),
        max_iterations=settings.get("max_iterations", 5),
        thinking_budget=settings.get("thinking_budget", 0),     # Phase 03-5
        show_thoughts=settings.get("show_thoughts", False),     # Phase 03-5
        search_service=search_service,
        embedding_service=embedding_service,
        embedding_repo=embed_repo if st.session_state.chunks else None,
        db_path=DB_PATH,
    )
```

---

## 9. ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ìœ í˜• | ë‚´ìš© |
|------|----------|------|
| `component/sidebar.py` | ìˆ˜ì • | `thinking_budget` ìŠ¬ë¼ì´ë”, `show_thoughts` í† ê¸€ ì¶”ê°€ |
| `service/react_graph.py` | ìˆ˜ì • | `__init__`ì— thinking íŒŒë¼ë¯¸í„°, `extract_thought_from_content()` ì¶”ê°€, `_parse_result()` í™•ì¥, `_parse_message_chunk()` thought ê°ì§€ |
| `component/chat_tab.py` | ìˆ˜ì • | `_handle_streaming_response`ì— thought ìˆ˜ì§‘ ë° expander í‘œì‹œ |
| `app.py` | ìˆ˜ì • | `_create_graph_builder()`ì— thinking íŒŒë¼ë¯¸í„° ì „ë‹¬ |

> ê¸°ì¡´ `build()`, `ChatState`, ê·¸ë˜í”„ êµ¬ì¡°ëŠ” **ë³€ê²½ ì—†ìŒ**.

---

## 10. ì¶”ê°€ë˜ëŠ” ì˜ì¡´ì„±

**ì—†ìŒ** â€” `langchain-google-genai` 4.2.0ì´ `thinking_budget`, `include_thoughts`ë¥¼ ë„¤ì´í‹°ë¸Œ ì§€ì›.

---

## 11. í…ŒìŠ¤íŠ¸ ê³„íš

### 11.1 thinking ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

```python
def test_thinking_mode():
    """thinking í™œì„±í™” í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        thinking_budget=1024,
        show_thoughts=True,
    )

    result = graph.invoke(
        "3x + 7 = 22 ì¼ ë•Œ xëŠ”?",
        session_id="test_thinking",
        turn_count=1,
    )

    assert result.get("thought_process")
    assert "5" in result["text"]


def test_thinking_disabled():
    """thinking ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        thinking_budget=0,
    )

    result = graph.invoke("ì•ˆë…•!", session_id="test", turn_count=1)
    assert not result.get("thought_process")


def test_thinking_with_tool_calling():
    """thinking + tool calling ë³‘í–‰ í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        thinking_budget=512,
        show_thoughts=True,
    )
    graph.build()

    result = graph.invoke(
        "ì§€ê¸ˆ ëª‡ ì‹œì•¼? ê·¸ë¦¬ê³  ì™œ ì‹œê°„ì´ ì¤‘ìš”í•œì§€ ë¶„ì„í•´ì¤˜",
        session_id="test_thinking_tool",
        turn_count=1,
    )

    assert "get_current_time" in result.get("tool_history", [])
    assert result.get("thought_process")


def test_unsupported_model_warning():
    """ë¯¸ì§€ì› ëª¨ë¸ì—ì„œ thinking_budget ë¬´ì‹œ í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.0-flash",
        thinking_budget=1024,
    )
    assert graph.thinking_budget == 0  # ìë™ ë¹„í™œì„±í™”
```

### 11.2 ìŠ¤íŠ¸ë¦¬ë° + thinking í…ŒìŠ¤íŠ¸

```python
def test_stream_with_thinking():
    """ìŠ¤íŠ¸ë¦¬ë° + thinking í†µí•© í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        thinking_budget=1024,
        show_thoughts=True,
    )
    graph.build()

    chunks = list(graph.stream(
        "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 10ë²ˆì§¸ í•­ì€?",
        session_id="test_stream_thinking",
        turn_count=1,
    ))

    # thought ì´ë²¤íŠ¸ ì¡´ì¬
    thought_chunks = [c for c in chunks if c["type"] == "thought"]
    assert len(thought_chunks) > 0

    # done ì´ë²¤íŠ¸
    done = chunks[-1]
    assert done["type"] == "done"
    assert "55" in done["metadata"]["text"]
```

### 11.3 UI í…ŒìŠ¤íŠ¸

1. ì¶”ë¡  ëª¨ë“œ í† ê¸€ ON
2. thinking_budget ìŠ¬ë¼ì´ë”ë¥¼ 1024ë¡œ ì„¤ì •
3. show_thoughts í† ê¸€ ON
4. ë³µì¡í•œ ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 10ë²ˆì§¸ í•­ì„ ê³„ì‚°í•´ì¤˜")
5. í™•ì¸:
   - ìŠ¤íŠ¸ë¦¬ë° ì¤‘ "ğŸ§  ì‚¬ê³  ì¤‘..." í‘œì‹œ
   - ì‘ë‹µ ì™„ë£Œ í›„ "ğŸ§  ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •" expander í‘œì‹œ
   - expander ë‚´ë¶€ì— ì¶”ë¡  ê³¼ì • í…ìŠ¤íŠ¸
   - ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œ ê²½ìš°ì—ë„ ì •ìƒ ë™ì‘
   - show_thoughts OFF ì‹œ expander ë¯¸í‘œì‹œ

---

## 12. ì£¼ì˜ì‚¬í•­

- thinking í™œì„±í™” ì‹œ **ì‘ë‹µ ì‹œê°„ ì¦ê°€** (thinking_budgetì— ë¹„ë¡€)
- thinking í† í°ë„ **ê³¼ê¸ˆ ëŒ€ìƒ** (`usage_metadata.output_token_details.reasoning`ìœ¼ë¡œ í™•ì¸)
- `gemini-2.0-flash`ì—ì„œ thinking_budget ì„¤ì • ì‹œ ì—ëŸ¬ â†’ ëª¨ë¸ ì²´í¬ ë¡œì§ìœ¼ë¡œ ë°©ì–´ (Section 5.2)
- thinking + bind_tools í˜¸í™˜ì€ `langchain-google-genai` 4.2.0ì—ì„œ í™•ì¸ë¨
