# Phase 03-5: ì¶”ë¡  ëª¨ë¸ (Thinking)

## ëª©í‘œ

Gemini 2.5ì˜ `thinking_budget` / `include_thoughts` ê¸°ëŠ¥ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ ë ˆë²¨ì—ì„œ ì¶”ë¡  ëª¨ë“œë¥¼ ì œì–´í•œë‹¤. í˜„ì¬ì˜ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì¶”ë¡  ë°©ì‹ê³¼ ë³‘í–‰í•œë‹¤.

---

## 1. í˜„ì¬ êµ¬ì¡°

### 1.1 í˜„ì¬ ì¶”ë¡  ëª¨ë“œ ì²˜ë¦¬

**íŒŒì¼**: `component/sidebar.py` (line 103-117)

```python
# Phase 02-7: ì¶”ë¡  ëª¨ë“œ ì„¤ì •
reasoning_mode = st.toggle(
    "ì¶”ë¡  ëª¨ë“œ (Reasoning Mode)",
    value=False,
    help="ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ì— gemini-2.5-pro ì‚¬ìš©",
)

auto_reasoning = st.toggle(
    "ìë™ ì¶”ë¡  ëª¨ë“œ ê°ì§€",
    value=True,
    help="ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì¶”ë¡  ëª¨ë“œ í™œì„±í™”",
)
```

**ë¬¸ì œì **:
- ëª¨ë¸ ë³€ê²½ë§Œ í•  ë¿, ì‹¤ì œ thinking ê¸°ëŠ¥ ë¯¸ì‚¬ìš©
- `thinking_budget`, `include_thoughts` ë¯¸ì ìš©

---

## 2. Gemini Thinking ê¸°ëŠ¥

### 2.1 thinking_budget

```python
from google.genai import types

# Gemini 2.5 ëª¨ë¸ì—ì„œ ì¶”ë¡  í† í° ì˜ˆì‚° ì„¤ì •
config = types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
        thinking_budget=1024  # ì¶”ë¡ ì— ì‚¬ìš©í•  í† í° ì˜ˆì‚°
        # 0: thinking ë¹„í™œì„±í™” (ë¹ ë¥¸ ì‘ë‹µ)
        # 128+: thinking í™œì„±í™” (gemini-2.5-pro ìµœì†Œ 128)
    )
)
```

### 2.2 thinking_level (Gemini 3)

```python
# Gemini 3 ëª¨ë¸ì—ì„œ ì‚¬ìš©
config = types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
        thinking_level=types.ThinkingLevel.HIGH  # MINIMAL, LOW, MEDIUM, HIGH
    )
)
```

### 2.3 ì¶”ë¡  ê³¼ì • ì ‘ê·¼

```python
for part in response.candidates[0].content.parts:
    if part.thought:
        print(f"[ì‚¬ê³  ê³¼ì •] {part.text}")
    else:
        print(f"[ì‘ë‹µ] {part.text}")
```

---

## 3. sidebar.py ìˆ˜ì •

### 3.1 thinking_budget ìŠ¬ë¼ì´ë” ì¶”ê°€

**ìœ„ì¹˜**: ì¶”ë¡  ëª¨ë“œ í† ê¸€ ë‹¤ìŒ

```python
# ê¸°ì¡´ í† ê¸€ ìœ ì§€
reasoning_mode = st.toggle(
    "ì¶”ë¡  ëª¨ë“œ (Reasoning Mode)",
    value=False,
    help="ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ì— thinking í™œì„±í™”",
)

auto_reasoning = st.toggle(
    "ìë™ ì¶”ë¡  ëª¨ë“œ ê°ì§€",
    value=True,
    help="ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì¶”ë¡  ëª¨ë“œ í™œì„±í™”",
)

# ì¶”ê°€: thinking_budget ì„¤ì •
if reasoning_mode:
    thinking_budget = st.slider(
        "Thinking Budget",
        min_value=0,
        max_value=8192,
        value=1024,
        step=128,
        help="ì¶”ë¡ ì— ì‚¬ìš©í•  í† í° ì˜ˆì‚° (0: ë¹„í™œì„±í™”, 128+: í™œì„±í™”)",
    )

    show_thoughts = st.toggle(
        "ì¶”ë¡  ê³¼ì • í‘œì‹œ",
        value=False,
        help="ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •ì„ UIì— í‘œì‹œ",
    )
else:
    thinking_budget = 0
    show_thoughts = False
```

### 3.2 ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ ìˆ˜ì •

```python
return {
    # ê¸°ì¡´ í•­ëª©ë“¤...
    "reasoning_mode": reasoning_mode,
    "auto_reasoning": auto_reasoning,
    "thinking_budget": thinking_budget,  # ì¶”ê°€
    "show_thoughts": show_thoughts,       # ì¶”ê°€
}
```

---

## 4. react_graph.py ìˆ˜ì •

### 4.1 __init__ íŒŒë¼ë¯¸í„° ì¶”ê°€

```python
def __init__(
    self,
    api_key: str,
    model: str = "gemini-2.0-flash",
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_output_tokens: int = 8192,
    seed: int = None,
    thinking_budget: int = 0,      # ì¶”ê°€
    show_thoughts: bool = False,   # ì¶”ê°€
    max_iterations: int = 5,
    # ...
):
    self.thinking_budget = thinking_budget
    self.show_thoughts = show_thoughts
    # ...
```

### 4.2 google-genai ì§ì ‘ ì‚¬ìš© (thinking ì§€ì›)

LangChain `ChatGoogleGenerativeAI`ëŠ” thinking_configë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” `google-genai` SDKë¥¼ ì§ì ‘ ì‚¬ìš©í•œë‹¤.

**ì¶”ê°€í•  import**:

```python
from google import genai
from google.genai import types
```

**ì¶”ë¡ ìš© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”**:

```python
def __init__(self, ...):
    # ê¸°ì¡´ LangChain LLM
    self._llm = ChatGoogleGenerativeAI(...)

    # google-genai í´ë¼ì´ì–¸íŠ¸ (thinking ì§€ì›)
    self._genai_client = genai.Client(api_key=api_key)
```

### 4.3 ì¶”ë¡  ëª¨ë“œ LLM í˜¸ì¶œ ë©”ì„œë“œ

```python
def _invoke_with_thinking(
    self,
    prompt: str,
    thinking_budget: int = None,
) -> tuple[str, str, int, int]:
    """thinking ëª¨ë“œë¡œ LLM í˜¸ì¶œ

    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        thinking_budget: ì¶”ë¡  í† í° ì˜ˆì‚° (Noneì´ë©´ self.thinking_budget ì‚¬ìš©)

    Returns:
        tuple: (ì‘ë‹µ í…ìŠ¤íŠ¸, ì‚¬ê³  ê³¼ì •, ì…ë ¥ í† í°, ì¶œë ¥ í† í°)
    """
    budget = thinking_budget if thinking_budget is not None else self.thinking_budget

    config = types.GenerateContentConfig(
        temperature=self.temperature,
        top_p=self.top_p,
        max_output_tokens=self.max_output_tokens,
    )

    # thinking_budget > 0 ì´ë©´ thinking í™œì„±í™”
    if budget > 0:
        config.thinking_config = types.ThinkingConfig(
            thinking_budget=budget
        )

    response = self._genai_client.models.generate_content(
        model=self.model_name,
        contents=prompt,
        config=config,
    )

    # ì‘ë‹µ íŒŒì‹±
    response_text = ""
    thought_text = ""

    for part in response.candidates[0].content.parts:
        if hasattr(part, "thought") and part.thought:
            thought_text += part.text + "\n"
        else:
            response_text += part.text

    # í† í° ì¶”ì 
    usage = response.usage_metadata
    input_tokens = usage.prompt_token_count if usage else 0
    output_tokens = usage.candidates_token_count if usage else 0

    return response_text.strip(), thought_text.strip(), input_tokens, output_tokens
```

### 4.4 _llm_node ìˆ˜ì • (ì¶”ë¡  ëª¨ë“œ ë¶„ê¸°)

```python
def _llm_node(self, state: ChatState) -> dict:
    """LLM ë…¸ë“œ: ë„êµ¬ í˜¸ì¶œ ë˜ëŠ” ìµœì¢… ì‘ë‹µ ìƒì„±"""
    messages = state.get("messages", [])

    # ì¶”ë¡  ëª¨ë“œ í™•ì¸
    use_thinking = self.thinking_budget > 0

    if use_thinking and not self._has_pending_tool_calls(state):
        # thinking ëª¨ë“œ: ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì—ë§Œ ì‚¬ìš©
        prompt = self._messages_to_prompt(messages, state)
        response_text, thought_text, in_tokens, out_tokens = self._invoke_with_thinking(prompt)

        # ì‘ë‹µ ë©”ì‹œì§€ ìƒì„±
        from langchain_core.messages import AIMessage
        response = AIMessage(content=response_text)

        # ì‚¬ê³  ê³¼ì • ì €ì¥ (UI í‘œì‹œìš©)
        return {
            "messages": [response],
            "thought_process": thought_text if self.show_thoughts else "",
            "input_tokens": state.get("input_tokens", 0) + in_tokens,
            "output_tokens": state.get("output_tokens", 0) + out_tokens,
        }
    else:
        # ê¸°ì¡´ LangChain LLM ì‚¬ìš© (ë„êµ¬ í˜¸ì¶œ í•„ìš”)
        system_content = self._build_system_prompt(state)
        all_messages = [SystemMessage(content=system_content)] + messages

        response = self._llm_with_tools.invoke(all_messages)

        in_tokens, out_tokens = 0, 0
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            in_tokens = response.usage_metadata.get("input_tokens", 0)
            out_tokens = response.usage_metadata.get("output_tokens", 0)

        return {
            "messages": [response],
            "input_tokens": state.get("input_tokens", 0) + in_tokens,
            "output_tokens": state.get("output_tokens", 0) + out_tokens,
        }


def _has_pending_tool_calls(self, state: ChatState) -> bool:
    """ë„êµ¬ í˜¸ì¶œ ëŒ€ê¸° ì¤‘ì¸ì§€ í™•ì¸"""
    messages = state.get("messages", [])
    if not messages:
        return False
    last_msg = messages[-1]
    return hasattr(last_msg, "tool_calls") and last_msg.tool_calls


def _messages_to_prompt(self, messages: list, state: ChatState) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    parts = []

    # System prompt
    parts.append(self._build_system_prompt(state))
    parts.append("")

    # ë©”ì‹œì§€ë“¤
    for msg in messages:
        role = msg.type if hasattr(msg, "type") else "unknown"
        content = msg.content if hasattr(msg, "content") else str(msg)
        parts.append(f"[{role}] {content}")

    return "\n".join(parts)
```

### 4.5 ChatState ìˆ˜ì •

```python
class ChatState(TypedDict):
    # ê¸°ì¡´ í•„ë“œë“¤...

    # ì¶”ê°€: ì‚¬ê³  ê³¼ì •
    thought_process: str
```

---

## 5. UIì—ì„œ ì‚¬ê³  ê³¼ì • í‘œì‹œ

### 5.1 chat_tab.py ìˆ˜ì •

**_handle_streaming_response í•¨ìˆ˜ì— ì¶”ê°€**:

```python
def _handle_streaming_response(on_stream: callable, user_input: str) -> dict:
    # ê¸°ì¡´ ì½”ë“œ...

    thought_process = ""

    for chunk in on_stream(user_input):
        chunk_type = chunk.get("type")

        if chunk_type == "thought":
            # ì‚¬ê³  ê³¼ì • ìˆ˜ì§‘
            thought_process += chunk.get("content", "")

        # ê¸°ì¡´ token, tool_call ë“± ì²˜ë¦¬...

    return {
        # ê¸°ì¡´ ë°˜í™˜ê°’...
        "thought_process": thought_process,  # ì¶”ê°€
    }
```

**ë©”ì‹œì§€ í‘œì‹œ ë¶€ë¶„ì— ì¶”ê°€**:

```python
if response:
    st.markdown(response.get("text", ""))

    # ì‚¬ê³  ê³¼ì • í‘œì‹œ (show_thoughtsê°€ Trueì¸ ê²½ìš°)
    if response.get("thought_process"):
        with st.expander("ğŸ§  ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •", expanded=False):
            st.markdown(response["thought_process"])

    # ê¸°ì¡´ ë„êµ¬ ì •ë³´ í‘œì‹œ...
```

---

## 6. ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ìœ í˜• | ë‚´ìš© |
|------|----------|------|
| `component/sidebar.py` | ìˆ˜ì • | thinking_budget, show_thoughts ì¶”ê°€ |
| `service/react_graph.py` | ìˆ˜ì • | google-genai thinking í†µí•© |
| `component/chat_tab.py` | ìˆ˜ì • | ì‚¬ê³  ê³¼ì • UI í‘œì‹œ |
| `app.py` | ìˆ˜ì • | íŒŒë¼ë¯¸í„° ì „ë‹¬ |

---

## 7. ì¶”ê°€ë˜ëŠ” ì˜ì¡´ì„±

**pyproject.toml**:

```toml
[project]
dependencies = [
    # ê¸°ì¡´...
    "google-genai>=1.0.0",  # thinking ì§€ì›
]
```

---

## 8. í…ŒìŠ¤íŠ¸ ê³„íš

### 8.1 thinking ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

```python
def test_thinking_mode():
    """thinking ëª¨ë“œ í™œì„±í™” í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-pro",
        thinking_budget=1024,
        show_thoughts=True,
    )

    result = graph.invoke(
        "3x + 7 = 22 ì¼ ë•Œ xëŠ”?",
        session_id="test"
    )

    # ì‚¬ê³  ê³¼ì •ì´ ìˆì–´ì•¼ í•¨
    assert result.get("thought_process")
    # ì •ë‹µ í¬í•¨
    assert "5" in result["text"]


def test_thinking_disabled():
    """thinking ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸"""
    graph = ReactGraphBuilder(
        api_key=os.getenv("GEMINI_API_KEY"),
        model="gemini-2.5-flash",
        thinking_budget=0,  # ë¹„í™œì„±í™”
    )

    result = graph.invoke("ì•ˆë…•!", session_id="test")

    # ì‚¬ê³  ê³¼ì • ì—†ìŒ
    assert not result.get("thought_process")
```

### 8.2 UI í…ŒìŠ¤íŠ¸

1. ì¶”ë¡  ëª¨ë“œ í† ê¸€ ON
2. thinking_budget ìŠ¬ë¼ì´ë” ì¡°ì ˆ
3. show_thoughts í† ê¸€ ON
4. ë³µì¡í•œ ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 10ë²ˆì§¸ í•­ì„ ê³„ì‚°í•´ì¤˜")
5. í™•ì¸:
   - ì‘ë‹µì— "ğŸ§  ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •" expander í‘œì‹œ
   - expander ë‚´ë¶€ì— ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì • í‘œì‹œ

---

## 9. ì°¸ê³ : Thinking ëª¨ë¸ ì œì•½ì‚¬í•­

| ëª¨ë¸ | thinking ì§€ì› | ìµœì†Œ budget |
|------|--------------|-------------|
| gemini-2.5-pro | O | 128 |
| gemini-2.5-flash | O | 0 (ë¹„í™œì„±í™” ê°€ëŠ¥) |
| gemini-2.0-flash | X | - |

**ì£¼ì˜ì‚¬í•­**:
- thinking í™œì„±í™” ì‹œ ì‘ë‹µ ì‹œê°„ ì¦ê°€
- thinking í† í°ë„ ê³¼ê¸ˆë¨
- temperature ì œì•½ ìˆì„ ìˆ˜ ìˆìŒ
