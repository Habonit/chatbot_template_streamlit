import streamlit as st
from pathlib import Path
import os
from dotenv import load_dotenv

from domain.message import Message
from domain.session import Session
from repository.conversation_repo import ConversationRepository
from repository.embedding_repo import EmbeddingRepository
from repository.pdf_extractor import PDFExtractor
from service.llm_service import LLMService
from service.embedding_service import EmbeddingService
from service.rag_service import RAGService
from service.summary_service import SummaryService
from service.search_service import SearchService
from service.tool_manager import ToolManager
from component.sidebar import render_sidebar
from component.chat_tab import render_chat_tab
from component.pdf_tab import render_pdf_tab

load_dotenv()

st.set_page_config(
    page_title="Gemini Hybrid Chatbot",
    page_icon="ğŸ¤–",
    layout="wide",
)

DATA_PATH = Path("data/sessions")
UPLOAD_PATH = Path("data/uploads/temp")
DATA_PATH.mkdir(parents=True, exist_ok=True)
UPLOAD_PATH.mkdir(parents=True, exist_ok=True)

TOKEN_LIMIT_K = int(os.getenv("TOKEN_LIMIT_K", "256"))
TOKEN_LIMIT = TOKEN_LIMIT_K * 1000


def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_session" not in st.session_state:
        st.session_state.current_session = Session.generate_id()
    if "sessions" not in st.session_state:
        st.session_state.sessions = [st.session_state.current_session]
    if "token_usage" not in st.session_state:
        st.session_state.token_usage = {"input": 0, "output": 0, "total": 0}
    if "summary" not in st.session_state:
        st.session_state.summary = ""
    if "chunks" not in st.session_state:
        st.session_state.chunks = []
    if "pdf_description" not in st.session_state:
        st.session_state.pdf_description = ""


def get_turn_count(messages: list[Message]) -> int:
    return len([m for m in messages if m.role == "user"])


def handle_chat_message(
    user_input: str,
    settings: dict,
    conv_repo: ConversationRepository,
    embed_repo: EmbeddingRepository,
) -> dict:
    if not settings.get("gemini_api_key"):
        return {"text": "Gemini API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "error": True}

    session_id = st.session_state.current_session

    llm_service = LLMService(
        api_key=settings["gemini_api_key"],
        default_model=settings["model"],
    )
    summary_service = SummaryService()
    tool_manager = ToolManager()

    tool_manager.register_switch_tool()

    if st.session_state.chunks:
        embed_service = EmbeddingService(
            api_key=settings["gemini_api_key"],
            model=settings["embedding_model"],
        )

        def search_pdf_knowledge(query: str, top_k: int = 5) -> list[dict]:
            query_embedding = embed_service.create_embedding(query)
            results = embed_repo.search_similar(session_id, query_embedding, top_k)
            return [{"content": r["chunk"].normalized_text, "score": r["score"]} for r in results]

        tool_manager.register_tool(search_pdf_knowledge)

    if settings.get("search_enabled") and settings.get("tavily_api_key"):
        search_service = SearchService(api_key=settings["tavily_api_key"])

        def web_search(query: str) -> str:
            results = search_service.search(
                query,
                search_depth=settings["search_depth"],
                max_results=settings["max_results"],
            )
            return search_service.format_for_llm(results)

        tool_manager.register_tool(web_search)

    turn_count = get_turn_count(st.session_state.messages) + 1

    user_msg = Message(turn_id=turn_count, role="user", content=user_input)
    st.session_state.messages.append(user_msg)
    conv_repo.append_message(session_id, user_msg)

    if summary_service.should_summarize(turn_count):
        to_summarize, to_keep = summary_service.get_turns_to_summarize(
            st.session_state.messages[:-1], turn_count - 1
        )
        if to_summarize:
            summary_prompt = summary_service.build_summary_prompt(
                st.session_state.summary, to_summarize
            )
            summary_result = llm_service.generate(summary_prompt, model="gemini-2.5-flash")
            st.session_state.summary = summary_result["text"]
    else:
        to_keep = st.session_state.messages[:-1]

    context = summary_service.build_context(
        messages=to_keep,
        summary=st.session_state.summary,
        system_prompt=_get_system_prompt(st.session_state.pdf_description),
    )

    full_prompt = f"{context}\n\n[í˜„ì¬ ì‚¬ìš©ì ì…ë ¥]\n{user_input}"

    current_tokens = st.session_state.token_usage["total"]
    if current_tokens >= TOKEN_LIMIT:
        return {"text": f"í† í° ì œí•œ({TOKEN_LIMIT_K}k)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.", "error": True}
    if current_tokens >= TOKEN_LIMIT * 0.8:
        st.warning(f"í† í° ì‚¬ìš©ëŸ‰ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ ({current_tokens:,}/{TOKEN_LIMIT:,})")

    model_to_use = settings["model"]
    result = llm_service.generate(
        full_prompt,
        model=model_to_use,
        tools=tool_manager.get_tools() if tool_manager.get_tool_names() else None,
        temperature=settings["temperature"],
        top_p=settings["top_p"],
    )

    if result.get("function_calls"):
        for fc in result["function_calls"]:
            if fc["name"] == "switch_to_reasoning":
                model_to_use = "gemini-2.5-pro"
                result = llm_service.generate(
                    full_prompt,
                    model=model_to_use,
                    tools=tool_manager.get_tools(),
                    temperature=settings["temperature"],
                    top_p=settings["top_p"],
                )
                break
            else:
                tool_result = tool_manager.execute_tool(fc["name"], fc["args"])
                enhanced_prompt = f"{full_prompt}\n\n[Tool Result: {fc['name']}]\n{tool_result}"
                result = llm_service.generate(
                    enhanced_prompt,
                    model=model_to_use,
                    temperature=settings["temperature"],
                    top_p=settings["top_p"],
                )

    assistant_msg = Message(
        turn_id=turn_count,
        role="assistant",
        content=result["text"],
        input_tokens=result["input_tokens"],
        output_tokens=result["output_tokens"],
        model_used=result["model_used"],
    )
    st.session_state.messages.append(assistant_msg)
    conv_repo.append_message(session_id, assistant_msg)

    st.session_state.token_usage["input"] += result["input_tokens"]
    st.session_state.token_usage["output"] += result["output_tokens"]
    st.session_state.token_usage["total"] += result["total_tokens"]

    return result


def handle_pdf_upload(uploaded_file, settings: dict) -> None:
    if uploaded_file:
        file_path = UPLOAD_PATH / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.session_state.uploaded_pdf = file_path


def handle_pdf_process(step: str, settings: dict) -> dict:
    if "uploaded_pdf" not in st.session_state:
        return {"error": "PDFê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    if not settings.get("gemini_api_key"):
        return {"error": "Gemini API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}

    pdf_path = st.session_state.uploaded_pdf
    session_id = st.session_state.current_session

    try:
        if step == "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...":
            extractor = PDFExtractor()
            text, pages = extractor.extract_text(pdf_path)
            st.session_state.extracted_text = text
            st.session_state.pdf_pages = pages

        elif step == "ì²­í‚¹ ì¤‘...":
            rag_service = RAGService()
            chunks = rag_service.chunk_text(
                st.session_state.extracted_text,
                chunk_size=1024,
                overlap=256,
                source_file=pdf_path.name,
            )
            st.session_state.chunks = chunks

        elif step == "ì •ê·œí™” ì¤‘...":
            llm_service = LLMService(api_key=settings["gemini_api_key"])
            for chunk in st.session_state.chunks:
                prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì •ê·œí™”í•˜ì„¸ìš”.
ê·œì¹™:
1. ì˜¤íƒˆìì™€ ë„ì–´ì“°ê¸° ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
2. ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ìì™€ ì¤‘ë³µ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
3. ì•½ì–´ê°€ ìˆë‹¤ë©´ ê´„í˜¸ ì•ˆì— í’€ì´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
4. í•µì‹¬ í‚¤ì›Œë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
5. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ë³€ê²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì›ë³¸ í…ìŠ¤íŠ¸:
{chunk.original_text}

ì •ê·œí™”ëœ í…ìŠ¤íŠ¸:"""
                result = llm_service.generate(prompt, model="gemini-2.5-flash")
                chunk.normalized_text = result["text"]

            desc_prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ PDF ë¬¸ì„œì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…(description)ì„ ì‘ì„±í•˜ì„¸ìš”.
50ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš© ìƒ˜í”Œ:
{st.session_state.chunks[0].normalized_text[:500] if st.session_state.chunks else ''}

ì„¤ëª…:"""
            desc_result = llm_service.generate(desc_prompt, model="gemini-2.5-flash")
            st.session_state.pdf_description = desc_result["text"]

        elif step == "ì„ë² ë”© ìƒì„± ì¤‘...":
            embed_service = EmbeddingService(
                api_key=settings["gemini_api_key"],
                model=settings["embedding_model"],
            )
            texts = [c.normalized_text for c in st.session_state.chunks]
            embeddings = embed_service.create_embeddings(texts)

            for chunk, embedding in zip(st.session_state.chunks, embeddings):
                chunk.embedding = embedding

            embed_repo = EmbeddingRepository(base_path=DATA_PATH)
            embed_repo.save_chunks(
                session_id,
                st.session_state.chunks,
                embedding_model=settings["embedding_model"],
                embedding_dim=768,
            )

        return {}

    except Exception as e:
        return {"error": str(e)}


def handle_pdf_delete(settings: dict) -> None:
    session_id = st.session_state.current_session
    embed_repo = EmbeddingRepository(base_path=DATA_PATH)
    embed_repo.delete_chunks(session_id)
    st.session_state.chunks = []
    st.session_state.pdf_description = ""


def _get_system_prompt(pdf_description: str = "") -> str:
    base_prompt = """ë‹¹ì‹ ì€ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:
1. ì¼ë°˜ ëª¨ë“œ: ê°„ë‹¨í•œ ì§ˆë¬¸, ì¼ìƒ ëŒ€í™”, ì •ë³´ ì¡°íšŒ
2. ì¶”ë¡  ëª¨ë“œ: ë³µì¡í•œ ë¶„ì„, ë‹¤ë‹¨ê³„ ì¶”ë¡ , ë¹„êµ/í‰ê°€, ìˆ˜í•™ì  ê³„ì‚°

ë‹¤ìŒ ìƒí™©ì—ì„œëŠ” ë°˜ë“œì‹œ switch_to_reasoning íˆ´ì„ í˜¸ì¶œí•˜ì„¸ìš”:
- ì—¬ëŸ¬ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê²°ë¡ ì„ ë„ì¶œí•´ì•¼ í•  ë•Œ
- "ì™œ", "ì–´ë–»ê²Œ", "ë¹„êµí•´ì¤˜", "ë¶„ì„í•´ì¤˜" ë“±ì˜ ì‹¬ì¸µ ì§ˆë¬¸
- PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ì´ í•„ìš”í•  ë•Œ
- ìˆ˜í•™ì  ê³„ì‚°ì´ë‚˜ ë…¼ë¦¬ì  ë‹¨ê³„ê°€ í•„ìš”í•  ë•Œ"""

    if pdf_description:
        base_prompt += f"""

[ì—…ë¡œë“œëœ PDF ì •ë³´]
{pdf_description}
ì‚¬ìš©ìê°€ ì´ ë¬¸ì„œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•˜ë©´ search_pdf_knowledge íˆ´ì„ ì‚¬ìš©í•˜ì„¸ìš”."""

    return base_prompt


def main():
    init_session_state()

    settings = render_sidebar()

    conv_repo = ConversationRepository(base_path=DATA_PATH)
    embed_repo = EmbeddingRepository(base_path=DATA_PATH)

    tab1, tab2 = st.tabs(["ğŸ’¬ Chat", "ğŸ“„ PDF Preprocessing"])

    with tab1:
        render_chat_tab(
            on_send=lambda msg: handle_chat_message(msg, settings, conv_repo, embed_repo),
            messages=st.session_state.messages,
        )

    with tab2:
        render_pdf_tab(
            on_upload=lambda f: handle_pdf_upload(f, settings),
            on_process=lambda step: handle_pdf_process(step, settings),
            on_delete=lambda: handle_pdf_delete(settings),
            chunks=st.session_state.chunks,
        )


if __name__ == "__main__":
    main()
